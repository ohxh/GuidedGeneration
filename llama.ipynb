{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardware\n",
    "\n",
    "These models run at half precision (fp16), so you'll need ~2x the parameter count in GPU ram bytes to run without further quantization. I.e. 7b -> 13 gb. A single A10 or A6000 is enough to get started. The largest model (65b) works on 4x A6000 (48gb each) or 2x A100 (80gb each).\n",
    "\n",
    "You can monitor graphics cards in a terminal with `nvidia-smi -l 1` (refresh every second). If the first GPU ram fills up and then you get a CUDA out of memory error, you may need to manually specify the max memory per card. \n",
    "\n",
    "### Models\n",
    "\n",
    "These models come from the huggingface project `decapoda-research`. Officially though, you're supposed to request access to the weights from Facebook. \n",
    "- [X] Submit this request [here](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform)\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "`pip install numpy pandas torch datasets transformers matplotlib scikit-learn sentencepiece tqdm accelerate matplotlib`\n",
    "\n",
    "There's a weird mismatch in the casing of llama (`llama` vs `LLaMA`) that breaks loading these models. The easiest fix I found is just pull some guy's forked version of the `transformers` package that fixes this.\n",
    "\n",
    "`pip install git+https://github.com/mbehm/transformers`\n",
    "\n",
    "### Further quantization\n",
    "\n",
    "`pip install bitsandbytes-cuda117` (replace 117 with your version)\n",
    "\n",
    "Should see error:\n",
    "\n",
    "`AttributeError: /home/ubuntu/anaconda3/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cget_col_row_stats`\n",
    "\n",
    "CD there and `cp libbitsandbytes_cuda117.so libbitsandbytes_cpu.so`! this actually works\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 17:04:31.473153: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebaf999667846488450567c1db626b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ff5a13b5584e8d940bbe50e33d89e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e63bb5e8968456580cbb01e017f2950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583cfdccc39547bb8c9d6406eee00feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/427 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f2a674e3064baca63aa1b8ed15917a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/25.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599d2baa7aed41efa69e9c7327d5c510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876e8b93aa80415aad07f33c2f35217d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26d1718f50747c9adb80463df6405ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e2123551e74c489be83b3ca1792322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980c2be6955242ea91d1735f732aefc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00004-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56470b407f85485d8bce03695dafbccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00005-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b3b682c19042d5914cf2f8f91be880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00006-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6fbefd0ec74a29821631d9e7248f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00007-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2460d2eb8aa644b492b004bee64e60a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00008-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070e3727bcd7460a8dd00c9cb70be288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00009-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccfdac7204c48e49e8f3e1012a48e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00010-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7763397e9c0d4c5896f5da304f01ca63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00011-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb823d0043fc4da5ac4524f4dbc557bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00012-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38567b970bf7411dadb8dd47ce553692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00013-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94cce013f1654621bd7c72df3ad3bab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00014-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6508ad3a385421884d24059285f841f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00015-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07f68adf8c0429e9ff62b41e0ab8ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00016-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368909590d404735be005e9bb4acb2a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00017-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8d8afeae534fa5a27eed49e33ddfcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00018-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7149822d12d14148bc85e483fd7d6acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00019-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295c7e43d74f4c8ba6d55681db7a1927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00020-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226ca699ef6e4649b2379db11a2ceded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00021-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa63a1c2d8d646eb9592e01df3f631d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00022-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f300ce3be3f453387eb7ae7181583a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00023-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011d253dc0644592b0fb1c7107a963af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00024-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858c61d2c221412dab6451c54a8b0314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00025-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80f3d8918574fa3bd4d679e8c2b8425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00026-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b058379ec940be947b03a54913da1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00027-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f20c70a5844aa4a8c61fe9ebdfc0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00028-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4563a280a62453f91c42302081b6a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00029-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d390e9f8524a958c0f24e762dd8e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00030-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c751ac1fac49078aa91b850583d738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00031-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e3550583f349a097ac307ee77bd2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00032-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f99fdfdd8449c6a00ec0fdc10076c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00033-of-00033.bin:   0%|          | 0.00/524M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc99a754979c4e1492b251e01462b7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c482b3b3a0fe4ab28e925d7c720af2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded llama-7b!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import LLaMATokenizer, LlamaForCausalLM\n",
    "\n",
    "MODELS = {\n",
    "  \"llama-7b\": \"decapoda-research/llama-7b-hf\",\n",
    "  \"llama-13b\": \"decapoda-research/llama-13b-hf\",\n",
    "  \"llama-30b\": \"decapoda-research/llama-30b-hf\",\n",
    "  \"llama-65b\": \"decapoda-research/llama-65b-hf\"\n",
    "}\n",
    "\n",
    "MODEL_TAG = \"llama-7b\"\n",
    "MODEL_NAME = MODELS[MODEL_TAG]\n",
    "\n",
    "tokenizer = LLaMATokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    # Further quantization (requries BitsAndBytes, experimental). Keep dtype=float16 with this\n",
    "    # load_in_8bit=LOAD_8BIT,\n",
    "    torch_dtype=torch.float16,\n",
    "    # `device_map` maps layers and the lm head to devices they live on. `auto` works, `sequential`\n",
    "    # and `balance_low0` should work but don't\n",
    "    device_map=\"auto\",\n",
    "    # If the first GPU ram fills up and then you get a CUDA out of memory error, you may need to\n",
    "    # manually specify the max memory per card. I don't know why accelerate / huggingface can't \n",
    "    # always infer this. For 4x A6000:\n",
    "    # max_memory = {0: \"44gib\", 1: \"44gib\", 2: \"44gib\", 3: \"44gib\"}\n",
    ")\n",
    "\n",
    "print(\"Loaded {}!\".format(MODEL_TAG))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "# Simple generation using huggingface's default interface. This will probably\n",
    "# produce output that's pretty bad since LLaMA is a foundation model and hasn't\n",
    "# been tuned on any downstream objective.\n",
    "\n",
    "# Also doesn't use any smart heuristic for stopping so it will just keep generating\n",
    "# until it hits the `max_new_tokens`\n",
    "def generate(\n",
    "    text,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=128,\n",
    "    **kwargs,\n",
    "):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "        **kwargs,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For messing around with generation\n",
    "\n",
    "# prefix = input()\n",
    "# completion = generate(prefix)\n",
    "# print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Extract LLaMA hidden states from some sequence of tokens `input_ids`.\n",
    "# Returns activations from the layer numbered `layer` (or -1 for last layer)\n",
    "def llama_hs_from_tokens(model, input_ids, layer=-1):\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, output_hidden_states=True)\n",
    "\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "    \n",
    "    return hs\n",
    "\n",
    "# Extract LLaMA hidden states from a string of text.\n",
    "# Optionally add an EOS token to the end of the input.\n",
    "# Returns activations from the layer numbered `layer` (or -1 for last layer)\n",
    "def llama_hs_from_text(model, tokenizer, text, layer=-1, add_eos=True):\n",
    "    if add_eos:\n",
    "      text = text + tokenizer.eos_token\n",
    "    \n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    return llama_hs_from_tokens(model, input_ids, layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset amazon_polarity (/home/fsuser/.cache/huggingface/datasets/amazon_polarity/amazon_polarity/3.0.0/a27b32b7e7b88eb274a8fa8ba0f654f1fe998a87c22547557317793b5d2772dc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0f56f543844241952f12de191f4803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the amazon polarity dataset. \n",
    "# This is stored on google drive (!) and sometimes the download throws\n",
    "# weird errors. You might need a new version of `datasets` or the drive\n",
    "# folder might be at its bandwidth limit for the day lol.\n",
    "data = load_dataset(\"amazon_polarity\")[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:36<00:00, 10.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make all training data\n",
    "neg_hs, pos_hs, y, all_text = make_training_data(model, tokenizer, data, format=format_amazon, n=400)\n",
    "\n",
    "# 50/50 train/test split\n",
    "n = len(y)\n",
    "neg_hs_train, neg_hs_test = neg_hs[:n//2], neg_hs[n//2:]\n",
    "pos_hs_train, pos_hs_test = pos_hs[:n//2], pos_hs[n//2:]\n",
    "text_train, text_test = all_text[:n//2], all_text[n//2:]\n",
    "y_train, y_test = y[:n//2], y[n//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Try simple logistic regression to see if that works.\n",
    "# Learn a plane that separates differences in the true direction vs differences\n",
    "# in the false direction.\n",
    "\n",
    "x_train = neg_hs_train - pos_hs_train\n",
    "x_test = neg_hs_test - pos_hs_test\n",
    "\n",
    "lr = LogisticRegression(class_weight=\"balanced\", max_iter=1000)\n",
    "lr.fit(x_train, y_train)\n",
    "print(\"Logistic regression accuracy: {}\".format(lr.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_next_from_set(\n",
    "#     text,\n",
    "#     options\n",
    "# ):\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "#     option_tokens = [tokenizer(x, return_tensors=\"pt\") for x in options]\n",
    "\n",
    "#     input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "#     option_ids = [x[\"input_ids\"].to(model.device) for x in option_tokens]\n",
    "\n",
    "\n",
    "#     # temperature=0.1,\n",
    "#     # top_p=0.75,\n",
    "#     # top_k=40,\n",
    "#     # num_beams=4,\n",
    "#     max_new_tokens=128,\n",
    "\n",
    "#     generation_config = GenerationConfig(\n",
    "#         temperature=temperature,\n",
    "#         top_p=top_p,\n",
    "#         top_k=top_k,\n",
    "#         num_beams=num_beams,\n",
    "#     )\n",
    "#     outputs = model.generate(input_ids, return_dict_in_generate=True, output_scores=True, max_new_tokens=max_new_tokens)\n",
    "\n",
    "#     transition_scores = model.compute_transition_scores(\n",
    "#         outputs.sequences, outputs.scores, normalize_logits=False\n",
    "#     )\n",
    "\n",
    "#     print(transition_scores)\n",
    "\n",
    "#     # outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=True, output_scores=True)\n",
    "#     # transition_scores = model.compute_transition_scores(\n",
    "#     #     outputs.sequences, outputs.scores, normalize_logits=True\n",
    "#     # )\n",
    "\n",
    "#     # input_length = inputs.input_ids.shape[1]\n",
    "#     # generated_tokens = outputs.sequences[:, input_length:]\n",
    "#     # for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "#     #   # | token | token string | logits | probability\n",
    "#     #   print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.4f} | {np.exp(score.numpy()):.2%}\")\n",
    "\n",
    "#     # with torch.no_grad():\n",
    "#     #   outputs = model(input_ids)\n",
    "\n",
    "#     # next_token_logits = outputs[\"logits\"]\n",
    "\n",
    "#     # print(outputs)\n",
    "\n",
    "#     # predictions = F.softmax(outputs.logits, dim=-1).detach().cpu()\n",
    "\n",
    "#     # print(predictions)\n",
    "\n",
    "#     # thresh = 1e-5\n",
    "#     # vocab_size = predictions.shape[-1]\n",
    "\n",
    "#     # # Predictions has one sentence (index 0) and we look at the last token predicted (-1)\n",
    "#     # idxs = torch.arange(0, vocab_size)[predictions[0][-1] >= thresh]\n",
    "#     # print(tokenizer.convert_ids_to_tokens(idxs))\n",
    "\n",
    "#     # with torch.no_grad():\n",
    "#     #     model()\n",
    "#     #     generation_output = model.generate(\n",
    "#     #         input_ids=input_ids,\n",
    "\n",
    "#     #         return_dict_in_generate=True,\n",
    "#     #         output_scores=True,\n",
    "#     #         max_new_tokens=max_new_tokens,\n",
    "#     #     )\n",
    "#     # s = generation_output.sequences[0]\n",
    "#     # output = tokenizer.decode(s)\n",
    "#     # return output\n",
    "\n",
    "# labels = []\n",
    "\n",
    "# for x in tqdm(text_test):\n",
    "#   text = format_amazon_for_completion(x)\n",
    "\n",
    "#   tag_ids = tokenizer([\"Positive\", \"Negative\"], add_special_tokens=False).input_ids \n",
    "\n",
    "#   completion =generate(text, force_words_ids=[tag_ids], max_new_tokens=5)\n",
    "\n",
    "\n",
    "#   result = completion[len(text):]\n",
    "#   labels.append(1 if \"Positive\" in result else 0)\n",
    "\n",
    "# print(labels)\n",
    "\n",
    "# predictions = np.array(labels)\n",
    "\n",
    "# acc = (predictions == y_test).mean()\n",
    "# acc = max(acc, 1 - acc)\n",
    "# print(\"Naive generation accuracy: {}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract out the mean (and maybe also normalize variance) of a data set\n",
    "def normalize(x, var_normalize = False):\n",
    "  normalized_x = x - x.mean(axis=0, keepdims=True)\n",
    "  if var_normalize:\n",
    "      normalized_x /= normalized_x.std(axis=0, keepdims=True)\n",
    "\n",
    "  return normalized_x\n",
    "\n",
    "# Collin's main loss function\n",
    "def informative_loss(p0, p1):\n",
    "  return (torch.min(p0, p1)**2).mean(0)\n",
    "\n",
    "def consistent_loss(p0, p1):\n",
    "  return ((p0 - (1-p1))**2).mean(0)\n",
    "\n",
    "def ccs_loss(p0, p1):\n",
    "  return informative_loss(p0,p1) + consistent_loss(p0,p1)\n",
    "\n",
    "def get_tensor_data(x0, x1):\n",
    "  x0 = torch.tensor(x0, dtype=torch.float, requires_grad=False, device=model.device)\n",
    "  x1 = torch.tensor(x1, dtype=torch.float, requires_grad=False, device=model.device)\n",
    "  return x0, x1\n",
    "\n",
    "# Learn the plane from CCS. For some reason this doesn't always converge so we'll\n",
    "# do `ntries` runs. 10 seems to be more than enough.\n",
    "# Returns [best_probe, best_loss]\n",
    "def ccs(x0, x1, nepochs=1000, ntries=10, lr=1e-3, verbose=False, weight_decay=0.01, var_normalize=False, loss_func=ccs_loss):\n",
    "    # Collin subtracts out the means before training\n",
    "    x0 = normalize(x0, var_normalize=var_normalize)\n",
    "    x1 = normalize(x1, var_normalize=var_normalize)\n",
    "\n",
    "    # Number of entries in the hidden states\n",
    "    d = x0.shape[-1]\n",
    "    \n",
    "    # Probe that we'll learn\n",
    "    probe = nn.Sequential(nn.Linear(d, 1),nn.Sigmoid())\n",
    "    probe.to(model.device)  \n",
    "    best_probe = copy.deepcopy(probe)\n",
    "      \n",
    "    best_loss = np.inf\n",
    "    \n",
    "    \n",
    "\n",
    "    for train_num in range(ntries):\n",
    "        # Make a new probe for this run\n",
    "        probe = nn.Sequential(nn.Linear(d, 1), nn.Sigmoid())\n",
    "        probe.to(model.device)  \n",
    "\n",
    "        # Order the data randomly in a tensor\n",
    "        x0, x1 = get_tensor_data(x0, x1)\n",
    "        permutation = torch.randperm(len(x0))\n",
    "        x0, x1 = x0[permutation], x1[permutation]\n",
    "        \n",
    "        # Set up optimizer. Collin uses adamW so that's what we'll go with\n",
    "        optimizer = torch.optim.AdamW(probe.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        # Start training\n",
    "        for epoch in range(nepochs):\n",
    "          # probe\n",
    "          p0, p1 = probe(x0), probe(x1)\n",
    "\n",
    "          # get the corresponding loss\n",
    "          loss = loss_func(p0, p1)\n",
    "\n",
    "          # update the parameters\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "        loss = loss.detach().cpu().item()\n",
    "        \n",
    "        if verbose:\n",
    "          print(\"Round {}: loss is {}\".format(train_num, loss))\n",
    "          \n",
    "        if loss < best_loss:\n",
    "            best_probe = copy.deepcopy(probe)\n",
    "            best_loss = loss\n",
    "\n",
    "    return best_probe, best_loss\n",
    "\n",
    "\n",
    "def predict_pairs(probe, x0, x1):\n",
    "  x0 = torch.tensor(x0, dtype=torch.float, requires_grad=False, device=model.device)\n",
    "  x1 = torch.tensor(x1, dtype=torch.float, requires_grad=False, device=model.device)\n",
    "  \n",
    "  with torch.no_grad():\n",
    "      p0, p1 = probe(x0), probe(x1)\n",
    "\n",
    "  avg_confidence = p0 - p1\n",
    "  predictions = (avg_confidence.detach().cpu().numpy())[:,0]\n",
    "  return predictions\n",
    "\n",
    "# Get raw credence scores (before sigmoid) from a single hidden state.\n",
    "# `hs` is a numpy array of hidden states to apply this to\n",
    "def classify_single(probe, hs):\n",
    "  # Extract the actual vectors\n",
    "  classifier_direction = np.squeeze(np.transpose(probe[0].weight.detach().cpu().numpy()))\n",
    "  confidences = np.apply_along_axis(lambda x : np.dot(x,classifier_direction), 1, hs)\n",
    "\n",
    "  return confidences\n",
    "\n",
    "def get_acc(probe, x0_test, x1_test, y_test):\n",
    "  predictions = (predict_pairs(probe, x0_test, x1_test) < 0.5).astype(int)\n",
    "\n",
    "  # If predictions get messed up (i.e. ever not 1 or 0) this method will show\n",
    "  # really good accuracy. TODO evaluate vs y_test and y_test inverted to avoid\n",
    "  # this.\n",
    "  acc = (predictions == y_test).mean()\n",
    "\n",
    "  acc = max(acc, 1 - acc)\n",
    "\n",
    "  return acc\n",
    "\n",
    "def is_reversed(probe, x0_test, x1_test, y_test):\n",
    "  predictions = (predict_pairs(probe, x0_test, x1_test) < 0.5).astype(int)\n",
    "\n",
    "  # If predictions get messed up (i.e. ever not 1 or 0) this method will show\n",
    "  # really good accuracy. TODO evaluate vs y_test and y_test inverted to avoid\n",
    "  # this.\n",
    "  acc = (predictions == y_test).mean()\n",
    "\n",
    "  return acc < 0.5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0: loss is 0.0008136031683534384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2212/1519275720.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x0 = torch.tensor(x0, dtype=torch.float, requires_grad=False, device=model.device)\n",
      "/tmp/ipykernel_2212/1519275720.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x1 = torch.tensor(x1, dtype=torch.float, requires_grad=False, device=model.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1: loss is 0.0007731855730526149\n",
      "Round 2: loss is 0.0007348332437686622\n",
      "Learned probe:\n",
      "\n",
      "[-0.0088816  -0.0011106  -0.05952156 ...  0.00654809  0.02589549\n",
      "  0.05425652]\n",
      "CCS Accuracy: 0.96, loss: 0.0007348332437686622\n"
     ]
    }
   ],
   "source": [
    "probe, loss = ccs(neg_hs_train, pos_hs_train, ntries=3, verbose=True)\n",
    "\n",
    "print(\"Learned probe:\\n\")\n",
    "print(np.squeeze(np.transpose(probe[0].weight.detach().cpu().numpy())))\n",
    "ccs_acc = get_acc(probe, neg_hs_test, pos_hs_test, y_test)\n",
    "\n",
    "print(\"CCS Accuracy: {}, loss: {}\".format(ccs_acc, loss))\n",
    "\n",
    "classifier_direction = np.squeeze(np.transpose(probe[0].weight.detach().cpu().numpy()))\n",
    "\n",
    "neg_credences = np.apply_along_axis(lambda x : np.dot(x,classifier_direction), 1, neg_hs_test)\n",
    "pos_credences = np.apply_along_axis(lambda x : np.dot(x,classifier_direction), 1, pos_hs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAExCAYAAACEbFMTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABcpklEQVR4nO3deXxM5/4H8M9MmIkkMrFkEUsWeyT2JkIiSgiiRIuIIlyX1lpSe6117aVu7a1bVK+lVLVFFUFtadReLS4au0QsSQgSSb6/P/zm1JjJRmKYfN6v17zIc57znO/ZZr5zzvOcUYmIgIiIiOg1pzZ3AEREREQFgUkNERERWQQmNURERGQRmNQQERGRRWBSQ0RERBaBSQ0RERFZBCY1REREZBGY1BAREZFFYFJDREREFoFJDVmEWbNmoUaNGsjKygIAXLx4ESqVCp988kmBLufcuXNo1aoVdDodVCoVNm3aVKDt09969eoFd3d3gzKVSoVJkyaZJZ6XTX8Mr1ixwtyh5MjUfjLF1PqMHj0afn5++Vre7Nmz4enpCSsrK9StWzd/wZLFY1LzHFasWAGVSqW8ihUrhvLly6NXr164du2aucMrclJSUjBz5kyMGjUKanXhHtKRkZH4/fffMXXqVKxatQoNGzYs1OURWbKhQ4fixIkT+OGHH/JUf/v27Rg5ciSaNGmC5cuXY9q0aYUcoWXI7UvepEmToFKpcOvWrVzb+vHHHxEUFAQnJyfY2NjA09MTXbp0wbZt2wo67OdSzNwBvM4+/vhjeHh44NGjR/j111+xYsUK7N+/H6dOnYK1tbW5wysyvvzyS2RkZCAiIqJQl/Pw4UPExMTgo48+wqBBgwp1WWTaw4cPUaxY0XjbcnNzw8OHD1G8eHFzh5KjL774QrlCml8uLi7o0KEDPvnkE7Rv3z7X+rt27YJarcZ//vMfaDSa51omPb9PPvkEI0aMQFBQEMaMGQMbGxucP38eO3fuxNq1a9G6dWtzh8ik5kW0adNG+ab+z3/+E2XLlsXMmTPxww8/oEuXLmaOzrwePXoEjUZT6FdOAGD58uVo3759oSeSiYmJAAAHB4cCazM1NRW2trYF1l5hevDgAWxsbMwaw+v0ZSEjIwNZWVnP/eGrUqlei/V90aSrS5cu6Ny5M/766y94enrmWPfmzZsoUaJErts0KysL6enpr8X2e11kZGRgypQpaNmyJbZv3240/ebNm2aIyhhvPxWgwMBAAMCFCxeUsvT0dEyYMAENGjSATqeDra0tAgMDsXv3boN569evj7ffftugzMfHByqVCidPnlTK1q1bB5VKhdOnT+cYy/z581GrVi3Y2NigVKlSaNiwIVavXm1Q59q1a+jTpw9cXV2h1Wrh4eGB/v37Iz09Xanz119/oXPnzihdujRsbGzQqFEjbNmyxaCdPXv2QKVSYe3atRg3bhzKly8PGxsbpKSkAABiY2PRunVr6HQ62NjYICgoCAcOHDBo4969exg6dCjc3d2h1Wrh5OSEli1b4ujRozmuZ1xcHE6ePIng4OBs63z66adwc3NDiRIlEBQUhFOnThnVOXPmDDp16oTSpUvD2toaDRs2NLgkPmnSJLi5uQEARowYAZVKZdCP4NixY2jTpg3s7e1hZ2eHFi1a4NdffzVYhv625S+//IIBAwbAyckJFSpUUKb/9NNPCAwMhK2tLUqWLInQ0FD88ccfOa7/0+0eOHAAUVFRcHR0hK2tLTp27KgkYk9btGgRatWqBa1WC1dXVwwcOBBJSUkGdZo1awZvb28cOXIETZs2hY2NDcaOHWtwGXvhwoXw9PSEjY0NWrVqhStXrkBEMGXKFFSoUAElSpRAhw4dcOfOHYO2v//+e4SGhirHXeXKlTFlyhRkZmbmuq5P96nRx5Ld62l5OQazc/PmTfTp0wfOzs6wtrZGnTp1sHLlSoM6T2+XefPmoXLlytBqtfjzzz+zbXfHjh0ICAiAg4MD7OzsUL16dYwdO9aozWf71Kxfvx5eXl6wtraGt7c3vvvuO6N+LQWxn4C8HSum+tQkJSWhV69e0Ol0cHBwQGRkpNF8evpz9/vvv892WwFP9v3y5cuRmpqq7GP9tlGpVBg0aBD++9//KvHqb4fk59zcv38/hgwZAkdHRzg4OOC9995Deno6kpKS0LNnT5QqVQqlSpXCyJEjISI5xgsA7u7uaNeuHbZv3466devC2toaXl5e2Lhxo1HdvLzXAnl7by8Mt27dQkpKCpo0aWJyupOTU6HHkBe8UlOALl68CAAoVaqUUpaSkoJly5YhIiICffv2xb179/Cf//wHISEhOHTokNLRLTAwEGvWrFHmu3PnDv744w+o1Wrs27cPtWvXBgDs27cPjo6OqFmzZrZxfPHFFxgyZAg6deqEDz74AI8ePcLJkycRGxuLbt26AQCuX78OX19fJCUloV+/fqhRowauXbuGDRs24MGDB9BoNEhISEDjxo3x4MEDDBkyBGXKlMHKlSvRvn17bNiwAR07djRY7pQpU6DRaDB8+HCkpaVBo9Fg165daNOmDRo0aICJEydCrVZj+fLlaN68Ofbt2wdfX18AwPvvv48NGzZg0KBB8PLywu3bt7F//36cPn0a9evXz3ZdDx48CADZ1vnqq69w7949DBw4EI8ePcK///1vNG/eHL///jucnZ0BAH/88QeaNGmC8uXLY/To0bC1tcU333yDsLAwfPvtt+jYsSPefvttODg4YNiwYYiIiEDbtm1hZ2enzB8YGAh7e3uMHDkSxYsXx9KlS9GsWTP88ssvRh0hBwwYAEdHR0yYMAGpqakAgFWrViEyMhIhISGYOXMmHjx4gMWLFyMgIADHjh3LU0fMwYMHo1SpUpg4cSIuXryIefPmYdCgQVi3bp1SZ9KkSZg8eTKCg4PRv39/nD17FosXL8Zvv/2GAwcOGHzrvn37Ntq0aYOuXbuie/fuyvYCgP/+979IT0/H4MGDcefOHcyaNQtdunRB8+bNsWfPHowaNQrnz5/H/PnzMXz4cHz55ZfKvCtWrICdnR2ioqJgZ2eHXbt2YcKECUhJScHs2bNzXU89R0dHrFq1yqDs8ePHGDZsmME3+bweg6Y8fPgQzZo1w/nz5zFo0CB4eHhg/fr16NWrF5KSkvDBBx8Y1F++fDkePXqEfv36QavVonTp0ibb/eOPP9CuXTvUrl0bH3/8MbRaLc6fP59rorVlyxaEh4fDx8cH06dPx927d9GnTx+UL1/eZP0X2U/5OVaeJiLo0KED9u/fj/fffx81a9bEd999h8jISJP1dTodKleujAMHDmDYsGHZrvuqVavw+eef49ChQ1i2bBkAoHHjxsr0Xbt24ZtvvsGgQYNQtmxZuLu75/vcHDx4MFxcXDB58mT8+uuv+Pzzz+Hg4ICDBw+iUqVKmDZtGrZu3YrZs2fD29sbPXv2zDZevXPnziE8PBzvv/8+IiMjsXz5cnTu3Bnbtm1Dy5YtASDP77V5eW/PyYMHD0z2m3nw4EGu8zo5OaFEiRL48ccfMXjw4GyPbbMTyrfly5cLANm5c6ckJibKlStXZMOGDeLo6CharVauXLmi1M3IyJC0tDSD+e/evSvOzs7yj3/8Qylbv369AJA///xTRER++OEH0Wq10r59ewkPD1fq1a5dWzp27JhjfB06dJBatWrlWKdnz56iVqvlt99+M5qWlZUlIiJDhw4VALJv3z5l2r1798TDw0Pc3d0lMzNTRER2794tAMTT01MePHhg0E7VqlUlJCREaVNE5MGDB+Lh4SEtW7ZUynQ6nQwcODDHmE0ZN26cAJB79+4ZlMfFxQkAKVGihFy9elUpj42NFQAybNgwpaxFixbi4+Mjjx49Moi9cePGUrVqVaM2Z8+ebbCssLAw0Wg0cuHCBaXs+vXrUrJkSWnatKlSpj9uAgICJCMjQym/d++eODg4SN++fQ3ajY+PF51OZ1T+LH27wcHBBtt52LBhYmVlJUlJSSIicvPmTdFoNNKqVStl34mILFiwQADIl19+qZQFBQUJAFmyZInBsvTbwNHRUWlXRGTMmDECQOrUqSOPHz9WyiMiIkSj0Rhs26ePEb333ntPbGxsDOpFRkaKm5ubQT0AMnHixGy3xYABA8TKykp27dolIvk7Bk2ZN2+eAJCvv/5aKUtPTxd/f3+xs7OTlJQUg+1ib28vN2/ezLFNEZFPP/1UAEhiYmK2dfRtLl++XCnz8fGRChUqGBzve/bsEQAG2+pF91N+jpVn99OmTZsEgMyaNUspy8jIkMDAQKP10WvVqpXUrFkz223x9LJsbW2NygGIWq2WP/74w6A8v+fms8eJv7+/qFQqef/99w3WpUKFChIUFJRrvG5ubgJAvv32W6UsOTlZypUrJ/Xq1VPK8vpem5f3dlP0x0Nur5yORxGRCRMmCACxtbWVNm3ayNSpU+XIkSP5jqcw8fbTCwgODoajoyMqVqyITp06wdbWFj/88IPBLQUrKyvlW2NWVhbu3LmDjIwMNGzY0ODWiv7W1d69ewE8uSLzxhtvoGXLlti3bx+AJ5d0T506pdTNjoODA65evYrffvvN5PSsrCxs2rQJb731lsnRO/pL91u3boWvry8CAgKUaXZ2dujXrx8uXrxodGk9MjISJUqUUP4+fvw4zp07h27duuH27du4desWbt26hdTUVLRo0QJ79+5VOhg6ODggNjYW169fz3HdnnX79m0UK1ZMuWryrLCwMINvsb6+vvDz88PWrVsBPLkitmvXLnTp0gX37t1TYrx9+zZCQkJw7ty5HEe0ZWZmYvv27QgLCzPoD1CuXDl069YN+/fvV27D6fXt2xdWVlbK3zt27EBSUhIiIiKU5d+6dQtWVlbw8/MzulWZnX79+hncdgkMDERmZiYuXboEANi5cyfS09MxdOhQg75Offv2hb29vdGlbq1Wi969e5tcVufOnaHT6ZS/9d94u3fvbtCR18/PD+np6Qbb8OljRL/NAwMD8eDBA5w5cyZP62rKV199hUWLFmHWrFl48803AeTvGDRl69atcHFxMeiEXrx4cQwZMgT379/HL7/8YlD/nXfegaOjY66x6vtlff/993nuZHv9+nX8/vvv6Nmzp8HxHhQUBB8fH5PzPO9+yu+x8rStW7eiWLFi6N+/v1JmZWWFwYMHZztPqVKl8jTyJidBQUHw8vJS/n6ec7NPnz4G55Cfnx9EBH369DFYl4YNG+Kvv/7KU1yurq4GV7Xt7e3Rs2dPHDt2DPHx8QDy/l6b23t7bvr164cdO3YYvXr06JGn+SdPnozVq1ejXr16+Pnnn/HRRx+hQYMGqF+/fq5dIl4WJjUvYOHChdixYwc2bNiAtm3b4tatW9BqtUb1Vq5cidq1a8Pa2hplypSBo6MjtmzZguTkZKWOs7MzqlatqiQw+/btQ2BgIJo2bYrr16/jr7/+woEDB5CVlZVrUjNq1CjY2dnB19cXVatWxcCBAw0uaycmJiIlJQXe3t45tnPp0iVUr17dqFx/60v/Yann4eFh8Pe5c+cAPEl2HB0dDV7Lli1DWlqasg1mzZqFU6dOoWLFivD19cWkSZPy/KaRk6pVqxqVVatWTblVeP78eYgIxo8fbxTjxIkTAeTcAS4xMREPHjzIdjtlZWXhypUrBuXZbafmzZsbxbB9+/Y8d8CrVKmSwd/626B3794F8Pf+ejZWjUYDT09Po/1Zvnz5bDtkPrss/QdnxYoVTZbrYwCe3Hrp2LEjdDod7O3t4ejoiO7duwOAwTmRH8ePH8f777+PiIgIREVFKeX5OQZNuXTpEqpWrWrU4T2v50B2wsPD0aRJE/zzn/+Es7Mzunbtim+++SbHBEe/rCpVqhhNM1UGPP9+yu+x8myc5cqVM/qiYeoc0RMRo35Q+fXstn+eczM/2+vpYzonVapUMVq3atWqAfi7y0Je32tze2/PTdWqVREcHGz0yq2D9tMiIiKwb98+3L17F9u3b0e3bt1w7NgxvPXWW3j06FGe2yks7FPzAnx9fZUrHWFhYQgICEC3bt1w9uxZ5YT++uuv0atXL4SFhWHEiBFwcnKClZUVpk+fbtChGAACAgIQHR2Nhw8f4siRI5gwYQK8vb3h4OCAffv24fTp07Czs0O9evVyjKtmzZo4e/YsNm/ejG3btuHbb7/FokWLMGHCBEyePLlwNgYMv4EDUN6gZ8+ene1DsvTbqUuXLggMDMR3332H7du3Y/bs2Zg5cyY2btyINm3aZLvMMmXKICMjA/fu3UPJkiXzHbM+xuHDhyMkJMRknew+MJ5Xdttp1apVcHFxMaqf1yHMT1/9eZrkoUOjKc/GmZdl5RZDUlISgoKCYG9vj48//hiVK1eGtbU1jh49ilGjRj3X0OC7d+/inXfeQbVq1ZS+Fnr5OQYLQk7b7Nl6e/fuxe7du7FlyxZs27YN69atQ/PmzbF9+/Zst2N+Pe9+etnu3r2LsmXLvlAbed32OcnP9jLHtjLXe7sp9vb2aNmyJVq2bInixYtj5cqViI2NRVBQ0EuN41lMagqIPlF58803sWDBAowePRoAsGHDBnh6emLjxo0G2br+KsDTAgMDsXz5cqxduxaZmZlo3Lgx1Go1AgIClKSmcePGeXrDs7W1RXh4OMLDw5Geno63334bU6dOxZgxY+Do6Ah7e3uTo4Ce5ubmhrNnzxqV628R6EcDZady5coAnhz8OY1O0itXrhwGDBiAAQMG4ObNm6hfvz6mTp2aY1JTo0YNAE9GQek7Uz9N/039af/73/+Ujrf6byjFixfPU4zPcnR0hI2NTbbbSa1WG33Le5Z+Ozk5OT1XDHml319nz541+GaWnp6OuLi4Ql223p49e3D79m1s3LgRTZs2Vcrj4uKeq72srCy8++67SEpKws6dO42GnOf3GHyWm5sbTp48iaysLIOrNXk9B3KiVqvRokULtGjRAnPnzsW0adPw0UcfYffu3SZj1S/r/PnzRtNMlb2IFzlW3NzcEB0djfv37xskjKbOEb24uDjUqVOnACL/W0GcmwVBfzX46ff///3vfwCgvA/l5702p/d2cw1hb9iwIVauXIkbN26YZflP4+2nAtSsWTP4+vpi3rx5ymU4fQLydFYfGxuLmJgYo/n1t5VmzpyJ2rVrK5c+AwMDER0djcOHD+d66wl40s/kaRqNBl5eXhARPH78GGq1GmFhYfjxxx9x+PBho/n1sbZt2xaHDh0yiDU1NRWff/453N3dDe5fm9KgQQNUrlwZn3zyCe7fv280XT/cODMz0+gWgJOTE1xdXZGWlpbjMvz9/QHA5HoAwKZNmwz6cxw6dAixsbFKouTk5IRmzZph6dKlJk9IU0Oin2ZlZYVWrVrh+++/Vy4lA09GM6xevRoBAQGwt7fPsY2QkBDY29tj2rRpePz4cb5jyKvg4GBoNBp89tlnBsfjf/7zHyQnJyM0NLRAlpMTU+dDeno6Fi1a9FztTZ48GT///DPWrFlj8tZPXo/B7LRt2xbx8fEGI8gyMjIwf/582NnZPfe3UlPDp/VXkrI75l1dXeHt7Y2vvvrKYF1++eUX/P77788VR3Ze5Fhp27YtMjIysHjxYqUsMzMT8+fPN1k/OTkZFy5cMBjJVBAK4twsCNevX8d3332n/J2SkoKvvvoKdevWVa7M5vW9Nrf39oJ0+fJlgz5uDx48MPm5BTx5HAVgeIvxzJkzuHz5coHGlBe8UlPARowYgc6dO2PFihV4//330a5dO2zcuBEdO3ZEaGgo4uLisGTJEnh5eRm9yVapUgUuLi44e/asQae6pk2bYtSoUQCQp6SmVatWcHFxQZMmTeDs7IzTp09jwYIFCA0NVW7RTJs2Ddu3b0dQUBD69euHmjVr4saNG1i/fj32798PBwcHjB49GmvWrEGbNm0wZMgQlC5dGitXrkRcXBy+/fbbXB+sp1arsWzZMrRp0wa1atVC7969Ub58eVy7dg27d++Gvb09fvzxR9y7dw8VKlRAp06dUKdOHdjZ2WHnzp347bffMGfOnByX4enpCW9vb+zcuRP/+Mc/jKZXqVIFAQEB6N+/P9LS0jBv3jyUKVMGI0eOVOosXLgQAQEB8PHxQd++feHp6YmEhATExMTg6tWrOHHiRI4x/Otf/1KeOTJgwAAUK1YMS5cuRVpaGmbNmpXjvMCTqwiLFy9Gjx49UL9+fXTt2hWOjo64fPkytmzZgiZNmmDBggW5tpMbR0dHjBkzBpMnT0br1q3Rvn17nD17FosWLcIbb7yh9GspTI0bN0apUqUQGRmJIUOGQKVSYdWqVc91Kf/333/HlClT0LRpU9y8eRNff/21wfTu3bvn+RjMTr9+/bB06VL06tULR44cgbu7OzZs2IADBw5g3rx5z3XLE3jyNPK9e/ciNDQUbm5uuHnzJhYtWoQKFSoYdBZ91rRp09ChQwc0adIEvXv3xt27d7FgwQJ4e3ubTNqe14scK2+99RaaNGmC0aNH4+LFi8pzWbLru7Rz505lGHhBe9FzsyBUq1YNffr0wW+//QZnZ2d8+eWXSEhIwPLly5U6eX2vzct7e0Hp2bMnfvnlF+XcfPDgARo3boxGjRqhdevWqFixIpKSkrBp0ybs27cPYWFhBl0jatasiaCgIOzZs6dA48rVyx5uZQn0w/9MDYfOzMyUypUrS+XKlSUjI0OysrJk2rRp4ubmJlqtVurVqyebN282OVxVRKRz584CQNatW6eUpaeni42NjWg0Gnn48GGu8S1dulSaNm0qZcqUEa1WK5UrV5YRI0ZIcnKyQb1Lly5Jz549laHonp6eMnDgQIMh6BcuXJBOnTqJg4ODWFtbi6+vr2zevNmgHf2Q7vXr15uM59ixY/L2228r8bi5uUmXLl0kOjpaRETS0tJkxIgRUqdOHSlZsqTY2tpKnTp1ZNGiRbmuq4jI3Llzxc7OzmCo8NPDr+fMmSMVK1YUrVYrgYGBcuLECaM2Lly4ID179hQXFxcpXry4lC9fXtq1aycbNmww2eazjh49KiEhIWJnZyc2Njby5ptvysGDBw3q5HTciDzZjiEhIaLT6cTa2loqV64svXr1ksOHD+e4/tm1q98vu3fvNihfsGCB1KhRQ4oXLy7Ozs7Sv39/uXv3rkGdoKAgk0NHs9sG2R0DpmI7cOCANGrUSEqUKCGurq4ycuRI+fnnn41izW1It36Z2b2eltsxmJOEhATp3bu3lC1bVjQajfj4+BgNS87p2DAlOjpaOnToIK6urqLRaMTV1VUiIiLkf//7n1Gbzy5r7dq1UqNGDdFqteLt7S0//PCDvPPOO1KjRo1c48nPfhLJ27Fiaj/dvn1bevToIfb29qLT6aRHjx5y7Ngxk+sTHh4uAQEBedhqOQ/pzu6REC9ybk6cONHkUOfs4niWm5ubhIaGys8//yy1a9cWrVYrNWrUMPlemZf32ry+tz8rt+PT1HrqH+ug9/jxY/niiy8kLCxM+TyzsbGRevXqyezZs40eXQIgT8PeC5rq/xdO9NpKTk6Gp6cnZs2aZTD0kqioqFu3LhwdHbFjxw5zh5Iv8fHx8PDwwNq1awvlSo25ubu7w9vbG5s3bzZ3KEUG+9TQa0+n02HkyJGYPXv2c/+wHtHr4PHjx8jIyDAo27NnD06cOIFmzZqZJ6gXMG/ePPj4+FhkQkPmwSs1RESviYsXLyI4OBjdu3eHq6srzpw5gyVLlkCn0+HUqVMoU6aMuUOkp/BKzcvHjsJERK+JUqVKoUGDBli2bBkSExNha2uL0NBQzJgxgwkNEXilhoiIiCwE+9QQERGRRWBSQ0RERBahyPSpycrKwvXr11GyZMkX/uE0IiIiejlEBPfu3YOrq2uuD30tMknN9evXX8rvfBAREVHBu3LlCipUqJBjnSKT1OgfIX3lypWX8nsfRERE9OJSUlJQsWLFPP0URJFJavS3nOzt7ZnUEBERvWby0nWEHYWJiIjIIjCpISIiIotQZG4/ERFR0eQ+eou5Q8i3izNCzR3Ca4lXaoiIiMgiMKkhIiIii8CkhoiIiCwCkxoiIiKyCExqiIiIyCJw9FNRNkln7gjyb1KyuSMgIqJXFK/UEBERkUVgUkNEREQWgUkNERERWQQmNURERGQRmNQQERGRReDoJyIiyrPX8XeUqOjglRoiIiKyCExqiIiIyCIwqSEiIiKL8FxJzcKFC+Hu7g5ra2v4+fnh0KFDOdZfv349atSoAWtra/j4+GDr1q0G03v16gWVSmXwat26tUGdO3fu4N1334W9vT0cHBzQp08f3L9//3nCJyIiIguU76Rm3bp1iIqKwsSJE3H06FHUqVMHISEhuHnzpsn6Bw8eREREBPr06YNjx44hLCwMYWFhOHXqlEG91q1b48aNG8przZo1BtPfffdd/PHHH9ixYwc2b96MvXv3ol+/fvkNn4iIiCyUSkQkPzP4+fnhjTfewIIFCwAAWVlZqFixIgYPHozRo0cb1Q8PD0dqaio2b96slDVq1Ah169bFkiVLADy5UpOUlIRNmzaZXObp06fh5eWF3377DQ0bNgQAbNu2DW3btsXVq1fh6uqaa9wpKSnQ6XRITk6Gvb19flbZcvG3n4gonzj66eW4OCPU3CG8MvLz+Z2vKzXp6ek4cuQIgoOD/25ArUZwcDBiYmJMzhMTE2NQHwBCQkKM6u/ZswdOTk6oXr06+vfvj9u3bxu04eDgoCQ0ABAcHAy1Wo3Y2Nj8rAIRERFZqHw9p+bWrVvIzMyEs7OzQbmzszPOnDljcp74+HiT9ePj45W/W7dujbfffhseHh64cOECxo4dizZt2iAmJgZWVlaIj4+Hk5OTYeDFiqF06dIG7TwtLS0NaWlpyt8pKSn5WVUiIiJ6zbwSD9/r2rWr8n8fHx/Url0blStXxp49e9CiRYvnanP69OmYPHlyQYVIREREr7h83X4qW7YsrKyskJCQYFCekJAAFxcXk/O4uLjkqz4AeHp6omzZsjh//rzSxrMdkTMyMnDnzp1s2xkzZgySk5OV15UrV3JdPyIiInp95Sup0Wg0aNCgAaKjo5WyrKwsREdHw9/f3+Q8/v7+BvUBYMeOHdnWB4CrV6/i9u3bKFeunNJGUlISjhw5otTZtWsXsrKy4OfnZ7INrVYLe3t7gxcRERFZrnwP6Y6KisIXX3yBlStX4vTp0+jfvz9SU1PRu3dvAEDPnj0xZswYpf4HH3yAbdu2Yc6cOThz5gwmTZqEw4cPY9CgQQCA+/fvY8SIEfj1119x8eJFREdHo0OHDqhSpQpCQkIAADVr1kTr1q3Rt29fHDp0CAcOHMCgQYPQtWvXPI18IiIiIsuX7z414eHhSExMxIQJExAfH4+6deti27ZtSmfgy5cvQ63+O1dq3LgxVq9ejXHjxmHs2LGoWrUqNm3aBG9vbwCAlZUVTp48iZUrVyIpKQmurq5o1aoVpkyZAq1Wq7Tz3//+F4MGDUKLFi2gVqvxzjvv4LPPPnvR9SciIiILke/n1Lyu+JwaE/icGiLKJz6n5uXgc2r+lp/P71di9BNRnjERIyKibPAHLYmIiMgiMKkhIiIii8CkhoiIiCwCkxoiIiKyCExqiIiIyCIwqSEiIiKLwKSGiIiILAKTGiIiIrIITGqIiIjIIjCpISIiIovApIaIiIgsApMaIiIisghMaoiIiMgiMKkhIiIii8CkhoiIiCwCkxoiIiKyCExqiIiIyCIwqSEiIiKLwKSGiIiILAKTGiIiIrIITGqIiIjIIjCpISIiIovApIaIiIgsApMaIiIisghMaoiIiMgiMKkhIiIii8CkhoiIiCwCkxoiIiKyCExqiIiIyCIwqSEiIiKL8FxJzcKFC+Hu7g5ra2v4+fnh0KFDOdZfv349atSoAWtra/j4+GDr1q3KtMePH2PUqFHw8fGBra0tXF1d0bNnT1y/ft2gDXd3d6hUKoPXjBkznid8IiIiskDF8jvDunXrEBUVhSVLlsDPzw/z5s1DSEgIzp49CycnJ6P6Bw8eREREBKZPn4527dph9erVCAsLw9GjR+Ht7Y0HDx7g6NGjGD9+POrUqYO7d+/igw8+QPv27XH48GGDtj7++GP07dtX+btkyZLPscpEL9kknbkjyL9JyeaOgIgo31QiIvmZwc/PD2+88QYWLFgAAMjKykLFihUxePBgjB492qh+eHg4UlNTsXnzZqWsUaNGqFu3LpYsWWJyGb/99ht8fX1x6dIlVKpUCcCTKzVDhw7F0KFD8xOuIiUlBTqdDsnJybC3t3+uNizO6/hhSy8HkxrKhvvoLeYOoUi4OCPU3CG8MvLz+Z2v20/p6ek4cuQIgoOD/25ArUZwcDBiYmJMzhMTE2NQHwBCQkKyrQ8AycnJUKlUcHBwMCifMWMGypQpg3r16mH27NnIyMjIto20tDSkpKQYvIiIiMhy5ev2061bt5CZmQlnZ2eDcmdnZ5w5c8bkPPHx8Sbrx8fHm6z/6NEjjBo1ChEREQYZ2ZAhQ1C/fn2ULl0aBw8exJgxY3Djxg3MnTvXZDvTp0/H5MmT87N6L4ZXPciSvI7H8+t4del13M5Ybe4AiLKV7z41henx48fo0qULRASLFy82mBYVFaX8v3bt2tBoNHjvvfcwffp0aLVao7bGjBljME9KSgoqVqxYeMETERGRWeUrqSlbtiysrKyQkJBgUJ6QkAAXFxeT87i4uOSpvj6huXTpEnbt2pXrfTM/Pz9kZGTg4sWLqF69utF0rVZrMtkhIiIiy5SvpEaj0aBBgwaIjo5GWFgYgCcdhaOjozFo0CCT8/j7+yM6Otqgg++OHTvg7++v/K1PaM6dO4fdu3ejTJkyucZy/PhxqNVqkyOuiIheB+6PeCuHqCDl+/ZTVFQUIiMj0bBhQ/j6+mLevHlITU1F7969AQA9e/ZE+fLlMX36dADABx98gKCgIMyZMwehoaFYu3YtDh8+jM8//xzAk4SmU6dOOHr0KDZv3ozMzEylv03p0qWh0WgQExOD2NhYvPnmmyhZsiRiYmIwbNgwdO/eHaVKlSqobUFERESvsXwnNeHh4UhMTMSECRMQHx+PunXrYtu2bUpn4MuXL0Ot/ntQVePGjbF69WqMGzcOY8eORdWqVbFp0yZ4e3sDAK5du4YffvgBAFC3bl2DZe3evRvNmjWDVqvF2rVrMWnSJKSlpcHDwwPDhg0z6DNDRERERVu+n1Pzuir059S8lqMYiCzIazj6ic98IUtTGM/XKbTn1BARERG9qpjUEBERkUVgUkNEREQWgUkNERERWQQmNURERGQRmNQQERGRRWBSQ0RERBaBSQ0RERFZBCY1REREZBGY1BAREZFFYFJDREREFoFJDREREVkEJjVERERkEZjUEBERkUVgUkNEREQWgUkNERERWQQmNURERGQRipk7ACKiAjFJZ+4InsNqcwdAZFF4pYaIiIgsApMaIiIisghMaoiIiMgiMKkhIiIii8CkhoiIiCwCkxoiIiKyCExqiIiIyCIwqSEiIiKLwIfvEZFFcH/EB9kRFXW8UkNEREQWgUkNERERWQTefiIqZLwtQkT0cvBKDREREVmE50pqFi5cCHd3d1hbW8PPzw+HDh3Ksf769etRo0YNWFtbw8fHB1u3bjWYLiKYMGECypUrhxIlSiA4OBjnzp0zqHPnzh28++67sLe3h4ODA/r06YP79+8/T/hERERkgfJ9+2ndunWIiorCkiVL4Ofnh3nz5iEkJARnz56Fk5OTUf2DBw8iIiIC06dPR7t27bB69WqEhYXh6NGj8Pb2BgDMmjULn332GVauXAkPDw+MHz8eISEh+PPPP2FtbQ0AePfdd3Hjxg3s2LEDjx8/Ru/evdGvXz+sXs1L+8/rdbwtctG6m7lDICKiV5RKRCQ/M/j5+eGNN97AggULAABZWVmoWLEiBg8ejNGjRxvVDw8PR2pqKjZv3qyUNWrUCHXr1sWSJUsgInB1dcWHH36I4cOHAwCSk5Ph7OyMFStWoGvXrjh9+jS8vLzw22+/oWHDhgCAbdu2oW3btrh69SpcXV1zjTslJQU6nQ7Jycmwt7fPzyrnzSRdwbdZyJjUvByv43YmInoeF2eEFnib+fn8zteVmvT0dBw5cgRjxoxRytRqNYKDgxETE2NynpiYGERFRRmUhYSEYNOmTQCAuLg4xMfHIzg4WJmu0+ng5+eHmJgYdO3aFTExMXBwcFASGgAIDg6GWq1GbGwsOnbsaLTctLQ0pKWlKX8nJycDeLJxCkVavnLDV0JW2gNzh5BvKSrj7Tzv1zR8fTIDh/raQK1S4VJSFmovScWUN7UY4qcpsGVfuJOFD7c/wpEbmUhJA/77tjXaVSue63yv43Y2t9s/L0Ta1T/h2mehUnZlXhfY+3WCzr+LGSN7OTKSb+LG8kEo3XIAbGs1M3c42TK1n0wxtT5J+/+LtCt/wDliWp6Xl3L4B9w/uR2Z9xJRvEwluHSf/SLhUyEojM9YfZt5ugYj+XDt2jUBIAcPHjQoHzFihPj6+pqcp3jx4rJ69WqDsoULF4qTk5OIiBw4cEAAyPXr1w3qdO7cWbp06SIiIlOnTpVq1aoZte3o6CiLFi0yudyJEycKAL744osvvvjiywJeV65cyTVPsdgh3WPGjDG4QpSVlYU7d+6gTJkyUKlUz9VmSkoKKlasCAD46KOP4ObmhkePHuG3337D6tWrUalSJfz6669KPyAypN9+V65cKbBbgIsWLcL06dNx7tw5ZbtfunQJtWvXxpQpUzBkyJACWc7Dhw/h4uKC4cOHY/z48QXSZn4VxvZ7VfXv3x/79+/H77//rpQ9evQIxYoVQ7Fiz/e29TptPxFBWloaihcvDisrK3OHA8D09nv8+DGysrKg1WpznFd/Ti5atAjvvvuuUt6rVy8kJCTgp59+ynX5EydOxGeffYaEhARoNAV3BfZlMucxuG/fPrRr1w4rV65EWFiY0fT+/fvj+++/x/Xr13NsZ/jw4fjiiy8QGhqKpk2bwsrKCufPn8eOHTvQuXNngzs5BUVEcO/evTx1NcnXu0PZsmVhZWWFhIQEg/KEhAS4uLiYnMfFxSXH+vp/ExISUK5cOYM6devWVercvHnToI2MjAzcuXMn2+VqtVqjE83BwSHnFcyHsLAwg9thrq6umDlzJvbs2YMuXSz/8nhOHj16BI1GA7Xa9OA6e3v7Ajuh16xZg/bt2xt0Ui9ZsiQAwNrausCWk5SUBABwdnYusDZTU1Nha2ub7/kKcvvl1YMHD2BjY/PSlle8eHGoVCqD9SyodX4Z2y8jIwNZWVmv7YdvTp5n++nPyRIlShjM++6776Jz5864desWPD09c2wjOTkZJUqUQNmyZXOsl5WVhfT09Ff6y6U5zmH9e42NjY3JZRcvXlyJLTsJCQlYtmwZ+vbti88//9xgmoggMTGx0NZLp9PlqV6+hnRrNBo0aNAA0dHRSllWVhaio6Ph7+9vch5/f3+D+gCwY8cOpb6HhwdcXFwM6qSkpCA2Nlap4+/vj6SkJBw5ckSps2vXLmRlZcHPzy8/q1BoAgMDAQAXLlxQytLT0zFhwgQ0aNAAOp0Otra2CAwMxO7duw3mrV+/Pt5++22DMh8fH6hUKpw8eVIpW7duHVQqFU6fPp1jLPPnz0etWrVgY2ODUqVKoWHDhkajxK5du4Y+ffrA1dUVWq0WHh4e6N+/P9LT05U6f/31Fzp37ozSpUvDxsYGjRo1wpYtWwza2bNnD1QqFdauXYtx48ahfPnysLGxUe6BxsbGonXr1tDpdEoC+uuvvxq0ce/ePQwdOhTu7u7QarVwcnJCy5YtcfTo0RzXMy4uDidPnjToj/WsTz/9FG5ubihRogSCgoJw6tQpozpnzpxBp06dULp0aVhbW6Nhw4b44YcflOmTJk2Cm5sbAGDEiBFQqVRwd3dXph87dgxt2rSBvb097Ozs0KJFC6N1XLFiBVQqFX755RcMGDAATk5OqFChgjL9p59+QmBgIGxtbVGyZEmEhobijz/+yHH9n273wIEDiIqKgqOjI2xtbdGxY0ckJiYa1V+0aBFq1aoFrVYLV1dXDBw4UEnY9Jo1awZvb28cOXIETZs2hY2NDcaOHYuLFy9CpVLhk08+wcKFC+Hp6QkbGxu0atUKV65cgYhgypQpqFChAkqUKIEOHTrgzp07Bm1///33CA0NVY67ypUrY8qUKcjMzMx1XVUqFSZNmgQASizZvZ6mPwb1V1nbtm2LAwcO5Lo8ALh58yb69OkDZ2dnWFtbo06dOli5cqVBnae3y7x581C5cmVotVr8+eef2ba7Y8cOBAQEwMHBAXZ2dqhevTrGjh1r1OaKFSsM5lu/fj28vLxgbW0Nb29vfPfdd+jVq5fB8VgQ+wkwPlY+/PBDozrPLht48gWgV69e0Ol0cHBwQGRkpNExpqc/d7///vtstxXwZN8vX74cqampyj7WbxuVSoVBgwbhv//9rxLvtm3bAOTv3Ny/fz+GDBkCR0dHODg44L333kN6ejqSkpLQs2dPlCpVCqVKlcLIkSPz1LfD3d0d7dq1w/79++Hr6wtra2vUrl3bZN2kpCQMHToUFStWhFarRZUqVTBz5kxkZWUZ1Lt9+zZ69OihPNokMjISJ06cMHmsFIa4uDiICJo0aWI0TaVSmRwB/dLleoPqGWvXrhWtVisrVqyQP//8U/r16ycODg4SHx8vIiI9evSQ0aNHK/UPHDggxYoVk08++UROnz4tEydOlOLFi8vvv/+u1JkxY4Y4ODjI999/LydPnpQOHTqIh4eHPHz4UKnTunVrqVevnsTGxsr+/fulatWqEhERkd/wX0hycrJyb++3334zmLZgwQIBIIsXL1bKEhMTpVy5chIVFSWLFy+WWbNmSfXq1aV48eJy7Ngxpd6QIUPE0dFR+fv27duiUqlErVbLggULlPKBAwca1DPl888/FwDSqVMnWbp0qfz73/+WPn36yJAhQ5Q6165dE1dXV7GxsZGhQ4fKkiVLZPz48VKzZk25e/euiIjEx8eLs7OzlCxZUj766COZO3eu1KlTR9RqtWzcuFFpa/fu3QJAvLy8pG7dujJ37lyZPn26pKamSnR0tGg0GvH395c5c+bI9OnTBYBoNBqJjY1V2ujWrZtoNBqJioqSZcuWycyZM+Wtt96Sr7/+Osd1/frrrwWAnDx50qA8Li5OAIiPj4+4u7vLzJkzZfLkyVK6dGlxdHRUjlURkVOnTolOpxMvLy+ZOXOmLFiwQJo2bSoqlUpZzxMnTsinn34qACQiIkJWrVol3333nTK/ra2tlCtXTqZMmSIzZswQDw8P0Wq18uuvvyrLWb58ubKdgoKCZP78+TJjxgwREfnqq69EpVJJ69atZf78+TJz5kxxd3cXBwcHiYuLU9rQH3/JyclG7darV0+aN28u8+fPlw8//FCsrKyUPml6+n5mwcHBMn/+fBk0aJBYWVnJG2+8Ienp6Uq9oKAgcXFxEUdHRxk8eLAsXbpUNm3apGzXunXripeXl8ydO1fGjRsnGo1GGjVqJGPHjpXGjRvLZ599JkOGDBGVSiW9e/c2iCEsLEy6dOkis2fPlsWLF0vnzp0FgAwfPtygXmRkpLi5uRmUAZCJEyeKiMj9+/dl1apVBq8vv/xSdDqdwTny9DE4depUASDe3t5Gx6ApDx48kJo1a0rx4sVl2LBh8tlnn0lgYKAAkHnz5in19NvFy8tLPD09ZcaMGfLpp5/KpUuXTLZ76tQp0Wg00rBhQ/n3v/8tS5YskeHDh0vTpk2N2ly+fLlStnnzZlGpVFK7dm2ZO3eujB8/XkqVKiXe3t4G26og9lN2xwoAuXXrVrb7KSsrS5o2bSpqtVoGDBgg8+fPl+bNm0vt2rWN1kevSpUq8s477+S4L1atWiWBgYGi1WqV/X3hwgUReXJc1KxZUxwdHWXy5MmycOFCOXbsWL7Pzbp160rr1q1l4cKF0qNHDwEgI0eOlICAAOnWrZssWrRI2rVrJwBk5cqVOcYrIuLm5ibVq1cXZ2dnGTt2rCxYsEDq1KkjAAyWn5qaKrVr15YyZcrI2LFjZcmSJdKzZ09RqVTywQcfKPUyMzPF399frKysZNCgQbJgwQJp2bKl0qapbfs0/Xv1l19+KYmJiUavrl27iq2tbY5tXL9+XQBIaGiopKam5roNzCHfSY2IyPz586VSpUqi0WjE19fXYAcFBQVJZGSkQf1vvvlGqlWrJhqNRmrVqiVbtmwxmJ6VlSXjx48XZ2dn0Wq10qJFCzl79qxBndu3b0tERITY2dmJvb299O7dW+7du/c84T+3R48eSYcOHQSA7Ny5UxITE+XKlSuyYcMGcXR0FK1Wa9CRKSMjQ9LS0gzauHv3rjg7O8s//vEPpWz9+vUCQP78808REfnhhx9Eq9VK+/btJTw8XKlXu3Zt6dixY44xdujQQWrVqpVjnZ49e4parTZKzESe7AsRkaFDhwoA2bdvnzLt3r174uHhIe7u7pKZmSkif58onp6e8uDBA4N2qlatKiEhIUqbjx49krFjx4q7u7u0bNlSqavT6WTgwIE5xmzKuHHjBIDRcaB/Uy9RooRcvXpVKY+NjRUAMmzYMKWsRYsW4uPjI48ePTKIvXHjxlK1alWjNmfPnm2wrLCwMNFoNMobrMiTE79kyZIGH1L6N86AgADJyMhQyu/duycODg7St29fg3bj4+NFp9MZlD969EgmTpxoEKu+3eDgYGU7i4gMGzZMrKysJCkpSUREbt68KRqNRlq1aqXsO5G/k/Evv/xSKQsKChIAsmTJEpPb1dHRUWlXRGTMmDECQOrUqSOPHz9WyiMiIkSj0RjE+/Qxovfee++JjY2NQb3ckhpTBgwYIFZWVrJr1y4RMT4G9dvv7t274uHhYXAMmjJv3jwBYJBcp6eni7+/v9jZ2UlKSorBdrG3t5ebN2/m2KaIKAlyYmJitnVMJTU+Pj5SoUIFg+N9z549AsBkUvO8+ym7Y0W/PZYuXaqUPbufNm3aJABk1qxZSllGRoaSDJr64G3VqpXUrFkz223x9LJMfegCELVaLX/88YdBeX7Pzaffq0RE/P39RaVSyfvvv2+wLhUqVJCgoKBc43VzcxMAsnfvXqXsypUrYmVlJUOHDlXKpkyZIra2tvK///3PYP7Ro0eLlZWVXL58WUREvv32W6OEOjMzU5o3b56vpCanV25JjciTzw8AUqpUKenYsaNyweJV8VxJTVGmPwGefbm7u8vPP/+c7XyZmZly+/ZtSUxMlNDQUKlbt64yLT4+3uBDZMSIERIQECDz588XV1dXEXmSDKnVapk7d26O8UVGRopOp5NDhw5lG4e9vb106NAhx3aqVatmckSb/mqL/kqb/kSZPHmyQb2jR48q32ie/Ubwz3/+U7RarfKG6ebmJg0bNpRr167lGNOz+vfvL8WKFTMq17+pm7qS5+fnJ9WrVxeRv6+ITZkyxSjGyZMnCwAlKTKV1GRkZIiNjY3RFRGRJx/UarVauaqiP26e/Ya3ceNGASC7du0yiqFVq1ZSpUqVHLeBvt1vvvnGZLsnTpwQEZHVq1cLANm6datBvbS0NLG3tzf4phwUFCRardYoIddvgwEDBhiU6z/Ink349B+CT3+oPC0lJUUSExOVK27Hjx9XpuU3qVm5cqUAkDlz5ihl+TkGTWnVqpW4uLgY1VmzZo0AkB9//NFguzx7tSM7+n22bNmybJf/bFKjH3k6duxYo7o+Pj4mk5rn3U/5OVae3U/9+vWTYsWKGX3R+Oabb7L94A0PD8/1CrR+WdklNW+++aZB2fOcm8+eQ/ovds9++QsLC5OKFSvmGq+bm5t4eXkZlT/75bR27drSunVro2N0586dBkl13759pXjx4kZXSPTJTl6TmgkTJsiOHTuMXq1atcpTUpOZmSkLFiyQ+vXrG3wGNm/e3OBLpLlY7OinwrZw4UJUq1YNycnJ+PLLL7F3716TIwBWrlyJOXPm4MyZM3j8+LFS7uHhofzf2dkZVatWxb59+/Dee+9h3759ePPNN9G0aVMMHjwYf/31F06fPo2srCyl7052Ro0ahZ07d8LX1xdVqlRBq1at0K1bN+UeaGJiIlJSUpSnOWfn0qVLJvsr1axZU5n+dBtPrw8A5WcuIiMjs11GcnIySpUqhVmzZiEyMhIVK1ZEgwYN0LZtW/Ts2TPXjoO5qVq1qlFZtWrV8M033wAAzp8/DxHB+PHjsx3RdPPmTZQvX97ktMTERDx48ADVq1c3mlazZk1kZWXhypUrqFWrllKe3XZq3ry5yWXktdNdpUqVDP4uVaoUAODu3bsAnuwvAEaxajQaeHp6KtP1ypcvn20n12eXpe/Ap++z8my5PgYA+OOPPzBu3Djs2rXL6HkW+mdJ5dfx48fx/vvvIyIiwmDEY36OQVMuXbqEqlWrGnV4f/oceNqz+zY74eHhWLZsGf75z39i9OjRaNGiBd5++2106tQp2871+mVVqVLFaFqVKlVM9j973v2U32Pl2TjLlSsHOzs7g3JT54ieiDz3iFS9Z7f985yb+dleTx/TOXm2TeDJufn0/OfOncPJkyfh6Ohosg39IBn9tn22w76pYyInPj4+Jvshfv3113maX61WY+DAgRg4cCBu376NAwcOYMmSJfjpp5/QtWtX7Nu3L1/xFDQmNc/J19dXGf0UFhaGgIAAdOvWDWfPnlVO6K+//hq9evVCWFgYRowYAScnJ1hZWWH69OkGHYoBICAgANHR0Xj48CGOHDmCCRMmwNvbGw4ODti3bx9Onz4NOzs71KtXL8e4atasibNnz2Lz5s3Ytm0bvv32WyxatAgTJkzA5MmTC2dj4MmohqfpO7jNnj1bGcX2LP126tKlCwIDA/Hdd99h+/btmD17NmbOnImNGzeiTZs22S6zTJkyyMjIwL1795TRFfmhj3H48OEICQkxWSe/bxi5yW47rVq1yuRIvrwOX85u2K/k74HhimfjzMuycoshKSkJQUFBsLe3x8cff4zKlSvD2toaR48exahRo4w6RebF3bt38c4776BatWpYtmyZwbT8HIMFIadt9my9vXv3Yvfu3diyZQu2bduGdevWoXnz5ti+fXuBDeF+3v30st29ezfXEU25yeu2z0l+tldet1VetnVWVhZatmyJkSNHmqxbrVq1PC3LHMqUKYP27dujffv2aNasGX755RdcunRJGVhhDkxqCoA+UXnzzTexYMEC5eciNmzYAE9PT2zcuNHgm8jEiRON2ggMDMTy5cuxdu1aZGZmonHjxlCr1QgICFCSmsaNG+fpDc/W1hbh4eEIDw9Heno63n77bUydOhVjxoyBo6Mj7O3tTY4CepqbmxvOnj1rVH7mzBllek4qV64M4MmVhpxGJ+mVK1cOAwYMwIABA3Dz5k3Ur18fU6dOzTGpqVGjBoAnPfJNjSp49kdRAeB///ufMlpDfyWoePHieYrxWY6OjrCxscl2O6nVaqNvec/SbycnJ6fniiGv9Pvr7NmzBlfA0tPTERcXV6jL1tuzZw9u376NjRs3omnTpkp5XFzcc7WXlZWFd999F0lJSdi5c6fRN9j8HoPPcnNzw8mTJ5GVlWVwBSWv50BO1Go1WrRogRYtWmDu3LmYNm0aPvroI+zevdtkrPplnT9/3miaqbIX8SLHipubG6Kjo3H//n2DhNHUOaIXFxeHOnXqFEDkfyuIc/NlqVy5Mu7fv5/rMerm5obdu3cbPV6hoPf/82rYsCF++eUX3Lhxw6xJzXP9SjcZa9asGXx9fTFv3jw8evQIwN9Z+tNZeWxsrMmflNDfVpo5cyZq166tXPoMDAxEdHQ0Dh8+nOutJ+DJkL+naTQaeHl5QUTw+PFjqNVqhIWF4ccff8Thw4eN5tfH2rZtWxw6dMgg1tTUVHz++edwd3eHl5dXjnE0aNAAlStXxieffGLy19T1w40zMzONbjs4OTnB1dXV4GcuTNEP+Te1HgCwadMmXLt2Tfn70KFDiI2NVRIlJycnNGvWDEuXLsWNGzeyjTE7VlZWaNWqFb7//ntcvHhRKU9ISMDq1asREBCQ6+2jkJAQ2NvbY9q0aQa3J/MaQ14FBwdDo9Hgs88+Mzge//Of/yA5ORmhoQX/ey3PMnU+pKenY9GiRc/V3uTJk/Hzzz9jzZo1Jm/95PUYzE7btm0RHx+PdevWKWUZGRmYP38+7OzsEBQU9Fxxmxo+rb+SlN0x7+rqCm9vb3z11VcG6/LLL78YPKCwILzIsdK2bVtkZGRg8eLFSllmZibmz59vsn5ycjIuXLiAxo0bF9wKoGDOzZelS5cuiImJwc8//2w0LSkpCRkZGQCevFc8fvwYX3zxhTI9KysLCxfm/BMVL+LChQsGdxXi4+NNPqogPT0d0dHRUKvVytXtx48f48yZMybfWwsTr9Tk0dSpU7Fly5ZsP0CBJx+agPGl0I4dOyI0NBRxcXFYsmQJvLy8jN5kq1SpAhcXF5w9exaDBw9Wyps2bYpRo0YBQJ6SmlatWsHFxQVNmjSBs7MzTp8+jQULFiA0NFS5RTNt2jRs374dQUFB6NevH2rWrIkbN25g/fr12L9/PxwcHDB69GisWbMGbdq0wZAhQ1C6dGmsXLkScXFx+Pbbb7O996+nVquxbNkytGnTBrVq1ULlypVx6dIlXL58GSKCNm3a4Mcff8S9e/dQoUIFdOrUCZUqVcJ3332H06dPIzMzE82aNUNGRka2t2A8PT3h7e2NnTt34h//+IfR9CpVqiAgIAD9+/dHWloa5s2bhzJlyhhc5l24cCECAgLg4+ODvn37wtPTEwkJCYiJicHVq1dx4sSJHNfzX//6l/LMkQEDBqBYsWJYunQp0tLSMGvWrNx2F+zt7bF48WL06NED9evXR9euXeHo6IjLly9jy5YtaNKkifLjsU9zd3c36NvwxhtvYPr06SZ/VBZ48s11zJgxmDx5Mlq3bo327dvj7NmzWLRoEd544w10794911hfVOPGjVGqVClERkZiyJAhUKlUWLVq1XPd9vj9998xZcoUNG3aFDdv3jTqD9C9e3ejY7BWrVqIjY1FcnIybGxsUKdOnRzv//fr1w9Lly5Fr169cOTIEbi7u2PDhg04cOAA5s2b91y3PAHg448/xt69exEaGgo3NzfcvHkTixYtQoUKFRAQEJDtfNOmTUOHDh3QpEkT9O7dG3fv3sWCBQvg7e1tMml7Xs8eK1ZWVgZP/P3HP/6BmTNnKlesnvbWW2+hSZMmGD16NC5evAgvLy9s3Lgx2/5SO3fuhIigQ4cOBRa/3ouem89r7969mD17Nq5evYpLly5h06ZNRk/wvXjxIsqVK4ekpCQ0atQItWrVQrt27dCrVy80aNAAqamp+P3337FhwwZcvHgRZcuWRVhYGHx9ffHhhx/i/PnzqFGjBn744QclSX7RfkmmtGjRQokXAK5evQpfX180b94cLVq0UB6Mu2bNGpw4cQJDhw5VbiVeu3YNNWvWRGRk5Et5ho7i5fdNfj1NmDBB5s6dKyEhISZ7xIs86YXv5OQk7u7ucvXqVbl+/bp8/PHH4ubmJlqtVurVqyebN282ObJDRJRndqxbt04pS09PFxsbG9FoNAbP7cnO0qVLpWnTplKmTBnRarVSuXJlGTFihMGzTURELl26JD179lSGont6esrAgQMNRrxcuHBBOnXqJA4ODmJtbS2+vr6yefNmg3b0PerXr19vMp5jx47J22+/LSVKlJBixYqJvb29FC9eXKKjo0XkyYiKESNGKM/AsbKykmrVqsnAgQOlbNmyMmbMmBzXd+7cuWJnZ2cwVPjpkUpz5syRihUrilarlcDAQGU00NMuXLggPXv2FBcXFylevLiUL19e2rVrJxs2bDDZ5rOOHj0qISEhYmdnJzY2NvLmm28a/T6afoSFqeNGvx1DQkJEp9OJtbW1VK5cWXr16iWHDx82Wd/NzU0+/vhjZeTKTz/9JPfv3zdoD4Ds3r3bYL4FCxZIjRo1pHjx4uLs7Cz9+/dXnk2kFxQUZPKxANltg+yOAVPrfODAAWnUqJGUKFFCXF1dZeTIkfLzzz8bxZrb6Kfchqc+7dixY+Lr6ysApFixYuLq6iqenp5ia2srCQkJpjavIiEhQXr37i1ly5YVjUYjPj4+RqNMcjo2TImOjpYOHTqIq6uraDQacXV1lYiICIMhvaaGdIs8eU5YjRo1RKvVire3t/zwww/yzjvvSI0aNXKNJz/7SeTvY0WtVkuxYsUkMjJSzpw5Izdu3FCGo5vaT7dv35YePXqIvb296HQ66dGjhxw7dszk+oSHh0tAQECetltOo5+yeyTEi5yb+mf1PDv0Prs49LZu3SofffSRODo6CgDlmVZ6Hh4eYmVlJZs2bZITJ05I+/btxc3NTUaMGCFVqlQRjUYjZcuWlcaNG8snn3xi8AypxMRE6datm5QsWVJ0Op306tVL+f3EtWvXZhuTSO7v1abWy83NzWD/pqSkyL///W8JCQmRChUqSPHixaVkyZLi7+8vX3zxhcGQeP1x+OwjXgobk5p8Wr58ueh0OpPTTB3AZCi77bd161ZRq9UGD8ZbvHix2NvbGw0tflpSUpKULl1ali1bVhjhvrLc3Nzk008/NXcYrw1fX1+DD77MzExxdXWV6dOnmzGqglGnTh0JDg4utPYnTpwoderUKfB2b9y4IdbW1rJp06YCb/tV8exnQlZWlri4uBgknElJSaLVamXNmjXPtYzvvvtOAMj+/ftfNFyLwD41BWzgwIEoW7YsfH198eWXX5ptRMHrJiYmBj4+PnB2dlbKQkJCkJKSkuPPBeh0OowcORKzZ89+rtEzr7MZM2agTJkyqFevHmbPnq3ceydD6enpOHLkiEFHTLVajeDgYJP9215Vjx8/NtrHe/bswYkTJ9CsWbNCXfa5c+fg6uoKT09PvPvuu7h8+fILtzlv3jz4+PgUyq2nV1VcXBzi4+MNjkWdTgc/P788HYsPHz40+FvfX8ne3h7169cv8HhfR+xTU4A+/vhjNG/eHDY2Nti+fTsGDBiA+/fvF9gvRVuy+Ph4g4QGgPJ3fHx8jvOOGjVK6XdUVAwZMgT169dH6dKlcfDgQYwZMwY3btzA3LlzzR3aK+fWrVvIzMw0eXyZ6hfyqrp27RqCg4PRvXt3uLq64syZM1iyZAlcXFzw/vvvF9py/fz8sGLFClSvXh03btzA5MmTERgYiFOnTj13vyLgSVJe1Ojfy0wdi7m9zwHA4MGD8fDhQ/j7+yMtLQ0bN27EwYMHMW3atAIZ1m4JinRSM3r0aMycOTPHOqdPn1aGDufm6Qe41atXD6mpqZg9e7bFJjUFvf2Kuvxsz6cfMle7dm1oNBq89957mD59usmHQNLrr1SpUmjQoAGWLVuGxMRE2NraIjQ0VLliV1iefqxC7dq14efnBzc3N3zzzTfo06dPoS2XjDVv3hxz5szB5s2b8ejRI1SpUgXz58/HoEGDzB3aK6NIJzUffvghevXqlWOdF3mqrZ+fH6ZMmYK0tDSL/KApyO3n4uKijB7TS0hIUKYVBS+yPf38/JCRkYGLFy/m+PTWoqhs2bKwsrJSjie9hISE1+rY0ul0BsPLzcXBwQHVqlV7ZZ6P8jrRH28JCQkoV66cUp6QkJDtAyKf1q1bN3Tr1q2wwrMIRTqpcXR0zPbR1AXh+PHjKFWqlEUmNEDBbj9/f39MnToVN2/eVH6+fseOHbC3t8/1mTiW4kW25/Hjx6FWq5VtR3/TaDRo0KABoqOjlaG1WVlZiI6O5jfc53D//n1cuHABPXr0MHcorx0PDw+4uLggOjpaSWJSUlIQGxuL/v37mzc4C1Gkk5r8uHz5Mu7cuYPLly8jMzMTx48fB/DkWSh2dnb48ccfkZCQgEaNGsHa2ho7duzAtGnTMHz4cPMG/orIbfu1atUKXl5e6NGjB2bNmoX4+HiMGzcOAwcOtNik8HnFxMQgNjYWb775JkqWLImYmBgMGzYM3bt3z/Z3jIq6qKgoREZGomHDhspDMlNTU9G7d29zh/bKGz58ON566y24ubnh+vXrmDhxIqysrBAREWHu0F5J9+/fN7iKFRcXh+PHj6N06dKoVKkShg4din/961+oWrUqPDw8MH78eLi6uho9y4aek7mHX70uIiMjTT4PQ/9sjZ9++knq1q0rdnZ2YmtrK3Xq1JElS5bk+CvARUlu209E5OLFi9KmTRspUaKElC1bVj788EN5/Pix+YJ+RR05ckT8/PyUZ9rUrFlTpk2bJo8ePTJ3aK+0+fPnS6VKlUSj0Yivr6/8+uuv5g7ptRAeHi7lypUTjUYj5cuXl/DwcDl//ry5w3plZfcMJf3zWrKysmT8+PHi7OwsWq1WWrRoIWfPnjVv0BZEJVI0xhxnZWXh+vXrKFmyZKE8eZGIiIgKnojg3r17cHV1zfVp9kXm9tP169dfmR8wIyIiovy5cuUKKlSokGOdIpPU6J+ncOXKlVfmh8yIiIgoZykpKahYsWKenotUZJIa/S0ne3t7JjVERESvmbx0HeHPJBAREZFFYFJDREREFqHI3H4iIqICMEln7gjyb1KyuSOgl4RXaoiIiMgiMKkhIiIii8CkhoiIiCwCkxoiIiKyCExqiIiIyCIwqSEiIiKLwKSGiIiILAKTGiIiIrIITGqIiIjIIjCpISIiIovApIaIiIgsApMaIiIisghMaoiIiMgiMKkhIiIii8CkhoiIiCwCkxoiIiKyCMXMHQAREVGhmqQzdwT5NynZ3BG8lnilhoiIiCwCkxoiIiKyCLz9VIS5j95i7hDy7eKMUHOHQEREryheqSEiIiKLwKSGiIiILAKTGiIiIrIITGqIiIjIIjCpISIiIovA0U/0WuGILSIiyg6v1BAREZFFYFJDREREFoFJDREREVkEJjVERERkEZjUEBERkUVgUkNEREQWgUkNERERWYRCS2oWLlwId3d3WFtbw8/PD4cOHcqx/vr161GjRg1YW1vDx8cHW7duNZjeq1cvqFQqg1fr1q0LK3wiIiJ6zRRKUrNu3TpERUVh4sSJOHr0KOrUqYOQkBDcvHnTZP2DBw8iIiICffr0wbFjxxAWFoawsDCcOnXKoF7r1q1x48YN5bVmzZrCCJ+IiIheQ4WS1MydOxd9+/ZF79694eXlhSVLlsDGxgZffvmlyfr//ve/0bp1a4wYMQI1a9bElClTUL9+fSxYsMCgnlarhYuLi/IqVapUYYRPREREr6ECT2rS09Nx5MgRBAcH/70QtRrBwcGIiYkxOU9MTIxBfQAICQkxqr9nzx44OTmhevXq6N+/P27fvp1tHGlpaUhJSTF4ERERkeUq8KTm1q1byMzMhLOzs0G5s7Mz4uPjTc4THx+fa/3WrVvjq6++QnR0NGbOnIlffvkFbdq0QWZmpsk2p0+fDp1Op7wqVqz4gmtGREREr7LX5gctu3btqvzfx8cHtWvXRuXKlbFnzx60aNHCqP6YMWMQFRWl/J2SksLEhoiIyIIV+JWasmXLwsrKCgkJCQblCQkJcHFxMTmPi4tLvuoDgKenJ8qWLYvz58+bnK7VamFvb2/wIiIiIstV4EmNRqNBgwYNEB0drZRlZWUhOjoa/v7+Jufx9/c3qA8AO3bsyLY+AFy9ehW3b99GuXLlCiZwIiIieq0VyuinqKgofPHFF1i5ciVOnz6N/v37IzU1Fb179wYA9OzZE2PGjFHqf/DBB9i2bRvmzJmDM2fOYNKkSTh8+DAGDRoEALh//z5GjBiBX3/9FRcvXkR0dDQ6dOiAKlWqICQkpDBWgYiIiF4zhdKnJjw8HImJiZgwYQLi4+NRt25dbNu2TekMfPnyZajVf+dTjRs3xurVqzFu3DiMHTsWVatWxaZNm+Dt7Q0AsLKywsmTJ7Fy5UokJSXB1dUVrVq1wpQpU6DVagtjFYiIiOg1oxIRMXcQL0NKSgp0Oh2Sk5PZv+b/uY/eYu4QioSLM0LNHQJRwZmkM3cERcOkZHNH8MrIz+c3f/uJiIiILMJrM6Sb6HX1Ol4R49UlInod8UoNERERWQQmNURERGQRmNQQERGRRWBSQ0RERBaBSQ0RERFZBI5+IiIietW8rs8DMvPzdXilhoiIiCwCkxoiIiKyCExqiIiIyCIwqSEiIiKLwI7CRETm8rp2BiV6RfFKDREREVkEJjVERERkEZjUEBERkUVgUkNEREQWgUkNERERWQQmNURERGQRmNQQERGRRWBSQ0RERBaBSQ0RERFZBCY1REREZBGY1BAREZFFYFJDREREFoFJDREREVkEJjVERERkEZjUEBERkUVgUkNEREQWgUkNERERWQQmNURERGQRmNQQERGRRShm7gCIiArEJJ25IyAiM+OVGiIiIrIITGqIiIjIIvD2UwFxH73F3CEQEREVabxSQ0RERBaBSQ0RERFZBN5+IiIjr+Pt1IvW5o6AiMyNV2qIiIjIIjCpISIiIovApIaIiIgsApMaIiIisghMaoiIiMgicPQTEVkE90erzR1Cvl207mbuEIgsCq/UEBERkUVgUkNEREQWgUkNERERWQQmNURERGQRmNQQERGRRWBSQ0RERBaBSQ0RERFZBCY1REREZBH48D0iIjPhAwOJClahXalZuHAh3N3dYW1tDT8/Pxw6dCjH+uvXr0eNGjVgbW0NHx8fbN261WC6iGDChAkoV64cSpQogeDgYJw7d66wwiciIqLXTKFcqVm3bh2ioqKwZMkS+Pn5Yd68eQgJCcHZs2fh5ORkVP/gwYOIiIjA9OnT0a5dO6xevRphYWE4evQovL29AQCzZs3CZ599hpUrV8LDwwPjx49HSEgI/vzzT1hbWxfGahARkQXgFbGiQyUiUtCN+vn54Y033sCCBQsAAFlZWahYsSIGDx6M0aNHG9UPDw9HamoqNm/erJQ1atQIdevWxZIlSyAicHV1xYcffojhw4cDAJKTk+Hs7IwVK1aga9euucaUkpICnU6H5ORk2NvbF9Ca/s199JYCb5OI6FXzOn7YMql5iSYlF3iT+fn8LvArNenp6Thy5AjGjBmjlKnVagQHByMmJsbkPDExMYiKijIoCwkJwaZNmwAAcXFxiI+PR3BwsDJdp9PBz88PMTExJpOatLQ0pKWlKX8nJz/Z0CkpKc+9bjnJSntQKO0SEb1KUlQF/j240L2O78+v43YGABTCZ6z+czsv12AKPKm5desWMjMz4ezsbFDu7OyMM2fOmJwnPj7eZP34+Hhlur4suzrPmj59OiZPnmxUXrFixbytCBERGdGZO4Dn0sXcAeTb67mdAcwovMjv3bsHnS7n9i129NOYMWMMrv5kZWXhzp07KFOmDFQqlRkje3WkpKSgYsWKuHLlSqHckqP84f54tXB/vFq4P14dL3tfiAju3bsHV1fXXOsWeFJTtmxZWFlZISEhwaA8ISEBLi4uJudxcXHJsb7+34SEBJQrV86gTt26dU22qdVqodVqDcocHBzysypFhr29Pd8kXiHcH68W7o9XC/fHq+Nl7ovcrtDoFfiQbo1GgwYNGiA6Olopy8rKQnR0NPz9/U3O4+/vb1AfAHbs2KHU9/DwgIuLi0GdlJQUxMbGZtsmERERFS2FcvspKioKkZGRaNiwIXx9fTFv3jykpqaid+/eAICePXuifPnymD59OgDggw8+QFBQEObMmYPQ0FCsXbsWhw8fxueffw4AUKlUGDp0KP71r3+hatWqypBuV1dXhIWFFcYqEBER0WumUJKa8PBwJCYmYsKECYiPj0fdunWxbds2paPv5cuXoVb/fZGocePGWL16NcaNG4exY8eiatWq2LRpk/KMGgAYOXIkUlNT0a9fPyQlJSEgIADbtm3jM2pegFarxcSJE41u05F5cH+8Wrg/Xi3cH6+OV3lfFMpzaoiIiIheNv6gJREREVkEJjVERERkEZjUEBERkUVgUkNEREQWgUlNETV16lQ0btwYNjY22T6U8PLlywgNDYWNjQ2cnJwwYsQIZGRkvNxAiyh3d3eoVCqD14wZM8wdVpGxcOFCuLu7w9raGn5+fjh06JC5QyqSJk2aZHQe1KhRw9xhFRl79+7FW2+9BVdXV6hUKuX3GPVEBBMmTEC5cuVQokQJBAcH49y5c+YJ9v8xqSmi0tPT0blzZ/Tv39/k9MzMTISGhiI9PR0HDx7EypUrsWLFCkyYMOElR1p0ffzxx7hx44byGjx4sLlDKhLWrVuHqKgoTJw4EUePHkWdOnUQEhKCmzdvmju0IqlWrVoG58H+/fvNHVKRkZqaijp16mDhwoUmp8+aNQufffYZlixZgtjYWNja2iIkJASPHj16yZE+RahIW758ueh0OqPyrVu3ilqtlvj4eKVs8eLFYm9vL2lpaS8xwqLJzc1NPv30U3OHUST5+vrKwIEDlb8zMzPF1dVVpk+fbsaoiqaJEydKnTp1zB0GiQgA+e6775S/s7KyxMXFRWbPnq2UJSUliVarlTVr1pghwid4pYZMiomJgY+Pj8Evo4eEhCAlJQV//PGHGSMrOmbMmIEyZcqgXr16mD17Nm/9vQTp6ek4cuQIgoODlTK1Wo3g4GDExMSYMbKi69y5c3B1dYWnpyfeffddXL582dwhEYC4uDjEx8cbnCs6nQ5+fn5mPVcs9le66cXEx8cbJDQAlL/j4+PNEVKRMmTIENSvXx+lS5fGwYMHMWbMGNy4cQNz5841d2gW7datW8jMzDR57J85c8ZMURVdfn5+WLFiBapXr44bN25g8uTJCAwMxKlTp1CyZElzh1ek6T8HTJ0r5vyM4JUaCzJ69GijTnXPvvjGbD752T9RUVFo1qwZateujffffx9z5szB/PnzkZaWZua1IHp52rRpg86dO6N27doICQnB1q1bkZSUhG+++cbcodErildqLMiHH36IXr165VjH09MzT225uLgYjfhISEhQplH+vcj+8fPzQ0ZGBi5evIjq1asXQnQEAGXLloWVlZVyrOslJCTwuH8FODg4oFq1ajh//ry5Qyny9OdDQkICypUrp5QnJCSgbt26ZoqKSY1FcXR0hKOjY4G05e/vj6lTp+LmzZtwcnICAOzYsQP29vbw8vIqkGUUNS+yf44fPw61Wq3sCyocGo0GDRo0QHR0NMLCwgAAWVlZiI6OxqBBg8wbHOH+/fu4cOECevToYe5QijwPDw+4uLggOjpaSWJSUlIQGxub7ajal4FJTRF1+fJl3LlzB5cvX0ZmZiaOHz8OAKhSpQrs7OzQqlUreHl5oUePHpg1axbi4+Mxbtw4DBw48JX8ZVZLEhMTg9jYWLz55psoWbIkYmJiMGzYMHTv3h2lSpUyd3gWLyoqCpGRkWjYsCF8fX0xb948pKamonfv3uYOrcgZPnw43nrrLbi5ueH69euYOHEirKysEBERYe7QioT79+8bXBWLi4vD8ePHUbp0aVSqVAlDhw7Fv/71L1StWhUeHh4YP348XF1dlS8EZmG2cVdkVpGRkQLA6LV7926lzsWLF6VNmzZSokQJKVu2rHz44Yfy+PFj8wVdRBw5ckT8/PxEp9OJtbW11KxZU6ZNmyaPHj0yd2hFxvz586VSpUqi0WjE19dXfv31V3OHVCSFh4dLuXLlRKPRSPny5SU8PFzOnz9v7rCKjN27d5v8nIiMjBSRJ8O6x48fL87OzqLVaqVFixZy9uxZs8asEhExV0JFREREVFA4+omIiIgsApMaIiIisghMaoiIiMgiMKkhIiIii8CkhoiIiCwCkxoiIiKyCExqiIiIyCIwqSEiIiKLwKSGiIiILAKTGiIiIrIITGqIiIjIIjCpISIiIovwfx2ee+NdWj12AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.title(\"Raw scores (before normalize or sigmoid) from pos H.S.\")\n",
    "plt.hist([pos_credences[[(not not x) for x in y_test]], pos_credences[[not x for x in y_test]]], stacked=True, density=True)\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.title(\"Raw scores (before normalize or sigmoid) from neg H.S.\")\n",
    "plt.hist([neg_credences[[(not not x) for x in y_test]], neg_credences[[not x for x in y_test]]], stacked=True, density=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For playing around with random statements\n",
    "\n",
    "\n",
    "# print(is_reversed(probe, neg_hs_test, pos_hs_test, y_test))\n",
    "\n",
    "# def sig(x):\n",
    "#  return 1/(1 + np.exp(-x))\n",
    "\n",
    "# while(True):\n",
    "#     t = input()\n",
    "#     hs = get_llama_hidden_states(model, tokenizer, t)\n",
    "#     raw = np.dot(classifier_direction, hs)\n",
    "#     print(\"'{}' scores {} (sigmoid {})\".format(t, raw, sig(raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2212/1519275720.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x0 = torch.tensor(x0, dtype=torch.float, requires_grad=False, device=model.device)\n",
      "/tmp/ipykernel_2212/1519275720.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x1 = torch.tensor(x1, dtype=torch.float, requires_grad=False, device=model.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCS accuracy (component 0): 0.93 in training, 0.955 in testing, 0.0006360188126564026 loss\n",
      "CCS accuracy (component 1): 0.575 in training, 0.54 in testing, 0.001127599854953587 loss\n",
      "CCS accuracy (component 2): 0.545 in training, 0.54 in testing, 0.0022196657955646515 loss\n",
      "CCS accuracy (component 3): 0.555 in training, 0.505 in testing, 0.002310703042894602 loss\n",
      "CCS accuracy (component 4): 0.55 in training, 0.515 in testing, 0.003679388901218772 loss\n"
     ]
    }
   ],
   "source": [
    "# Component of x perpendicular to y\n",
    "def perp(x,y):\n",
    "  along = y * (np.dot(x,y) / np.dot(y,y))\n",
    "  return x - along\n",
    "\n",
    "residual_neg_hs_train = neg_hs_train\n",
    "residual_pos_hs_train = pos_hs_train\n",
    "\n",
    "residual_neg_hs_test = neg_hs_test\n",
    "residual_pos_hs_test = pos_hs_test\n",
    "\n",
    "accs = []\n",
    "train_accs = []\n",
    "losses = []\n",
    "probes = []\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "  probe, loss = ccs(residual_neg_hs_train, residual_pos_hs_train, ntries=5, loss_func=ccs_loss)\n",
    "\n",
    "  ccs_acc = get_acc(probe, residual_neg_hs_test, residual_pos_hs_test, y_test)\n",
    "  ccs_train_acc = get_acc(probe, residual_neg_hs_train, residual_pos_hs_train, y_train)\n",
    "  \n",
    "  print(\"CCS accuracy (component {}): {} in training, {} in testing, {} loss\".format(i,ccs_train_acc,ccs_acc, loss))\n",
    "\n",
    "  train_accs.append(ccs_train_acc)\n",
    "  accs.append(ccs_acc)\n",
    "  losses.append(loss)\n",
    "  probes.append(probe)\n",
    "\n",
    "  # The direction we just found that best classifies the data\n",
    "  classifier_direction = np.squeeze(np.transpose(probe[0].weight.detach().cpu().numpy()))\n",
    "\n",
    "  residual_neg_hs_train = np.apply_along_axis(lambda x : perp(x,classifier_direction), 1, residual_neg_hs_train)\n",
    "  residual_pos_hs_train = np.apply_along_axis(lambda x : perp(x,classifier_direction), 1, residual_pos_hs_train)\n",
    "  residual_neg_hs_test = np.apply_along_axis(lambda x : perp(x,classifier_direction), 1, residual_neg_hs_test)\n",
    "  residual_pos_hs_ttest = np.apply_along_axis(lambda x : perp(x,classifier_direction), 1, residual_pos_hs_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.plot(accs, label='Acc')\n",
    "plt.plot(train_accs, label='Train Acc')\n",
    "plt.plot(np.array(losses) * 10, label='Train CCS Loss * 10')\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weird stuff going on here...\n",
    "\n",
    "- Potentially overfitting?\n",
    "- Training accuracy can be slightly worse (!) than test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hoverable view (broken in this notebook)\n",
    "# def plot_component_classification(preds1, preds2, y_real, texts):\n",
    "#   fig = plt.figure()\n",
    "#   ax = fig.add_subplot()\n",
    "\n",
    "#   # Make the grid\n",
    "#   x, y = np.meshgrid(np.arange(-1, 1, 0.1),\n",
    "#                         np.arange(-1, 1, 0.1))\n",
    "    \n",
    "#   colors = [\"red\" if y == 0  else \"green\" for y in y_real]\n",
    "#   sc = ax.scatter(preds1, preds2, color=colors)\n",
    "    \n",
    "#   # setting title and labels\n",
    "#   ax.set_title(\"Test Classsification\")\n",
    "#   ax.set_xlabel('Component 0')\n",
    "#   ax.set_ylabel('Component 1')\n",
    "\n",
    "#   annot = ax.annotate(\"\", xy=(0,0), xytext=(4,4),textcoords=\"offset points\",\n",
    "#                       bbox=dict(boxstyle=\"round\", fc=\"w\"),\n",
    "#                       arrowprops=dict(arrowstyle=\"->\"))\n",
    "#   annot.set_color(\"white\")\n",
    "#   annot.set_fontsize(8)\n",
    "#   annot.set_wrap(True)\n",
    "#   annot.get_bbox_patch().set_facecolor(\"black\")\n",
    "#   annot.get_bbox_patch().set_alpha(0.8)\n",
    "#   annot.set_visible(False)\n",
    "  \n",
    "#   def update_annot(ind):\n",
    "#       pos = sc.get_offsets()[ind[\"ind\"][0]]\n",
    "#       annot.xy = pos\n",
    "#       text = \"{}, {}\".format(\" \".join(list(map(str,ind[\"ind\"]))), \n",
    "#                             \" \".join([texts[n] for n in ind[\"ind\"]]))\n",
    "#       annot.get_bbox_patch().set_facecolor(\"red\" if y_real[ind[\"ind\"][0]] == 0 else \"green\")\n",
    "#       annot.set_text(text)\n",
    "    \n",
    "#   def hover(event):\n",
    "#       vis = annot.get_visible()\n",
    "#       if event.inaxes == ax:\n",
    "#           cont, ind = sc.contains(event)\n",
    "#           if cont:\n",
    "#               update_annot(ind)\n",
    "#               annot.set_visible(True)\n",
    "#               fig.canvas.draw_idle()\n",
    "#           else:\n",
    "#               if vis:\n",
    "#                   annot.set_visible(False)\n",
    "#                   fig.canvas.draw_idle()\n",
    "\n",
    "#   fig.canvas.mpl_connect(\"motion_notify_event\", hover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "preds1 = predict_pairs(probes[0], pos_hs_test, neg_hs_test)\n",
    "preds2 =  predict_pairs(probes[1], pos_hs_test, neg_hs_test)\n",
    "# preds3 =   predict(probes[1], pos_hs_test, neg_hs_test)\n",
    "\n",
    "# preds4 = predict(probes[0], pos_hs_train, neg_hs_train)\n",
    "# preds5 = predict(probes[0], pos_hs_train, neg_hs_train)\n",
    "# preds6 = predict(probes[0], pos_hs_train, neg_hs_train)\n",
    "plot_component_classification(preds1, preds2, y_test, text_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples = [\n",
    "#     # \"The sky is red.\",\n",
    "#     # \"The sky is blue.\",\n",
    "#     # \"Roses are red.\",\n",
    "#     # \"Roses are blue.\",\n",
    "#     # \"France is in Europe.\",\n",
    "#     # \"The United States is in Europe.\",\n",
    "#     # \"France is in Asia.\",\n",
    "#     # \"The United States is in Asia.\",\n",
    "#     # \"George Washington was the first president.\",\n",
    "#     # \"George Strait was the first president.\",\n",
    "#     # \"George Washington was a country singer.\",\n",
    "#     # \"George Strait was a country singer.\",\n",
    "#     # \"China is in Asia\",\n",
    "#     # \"Japan is in Asia\",\n",
    "#     \"The sky is red\",\n",
    "#     \"The sky is orange\",\n",
    "#     \"The sky is yellow\",\n",
    "#     \"The sky is green\",\n",
    "#     \"The sky is blue\",\n",
    "#     \"The sky is purple\",\n",
    "#     \"The sky is black\",\n",
    "# ]\n",
    "\n",
    "# true_ex = [\"Evaluate the following statement: \" + x + \" This statement is true.\" for x in examples]\n",
    "# false_ex = [\"Evaluate the following statement: \" + x + \" This statement is false.\" for x in examples]\n",
    "# labels = [0,1,1,0,1,0,0,0,1,0,0,1,1,1]\n",
    "\n",
    "# true_hs = np.array([get_llama_hidden_states(model, tokenizer, x) for x in true_ex])\n",
    "# false_hs = np.array([get_llama_hidden_states(model, tokenizer, x) for x in false_ex])\n",
    "\n",
    "# classifications1 = predict(probes[0],true_hs, false_hs)\n",
    "# classifications2 = predict(probes[1], true_hs, false_hs)\n",
    "\n",
    "\n",
    "# plot_component_classification(classifications1, classifications2,  labels, examples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The capital of the United States is Washington, D.C.\n",
      "Washington, D\n",
      "Prediction\n",
      "[-0.6743  0.2886 -0.6533 ... -0.8594 -0.3892  1.9795]\n",
      "[0.04220039]\n",
      "29892 | ,        | -0.875 | 41.70%\n",
      "  360 |  D       | -0.380 | 68.36%\n",
      "29889 | .        | -0.002 | 99.76%\n",
      "29907 |  C       | -0.036 | 96.44%\n",
      "29889 | .        | -0.419 | 65.77%\n",
      "   13 |  <0x0A>  | -1.965 | 14.01%\n",
      "29956 |  W       | -13.352 | 0.00%\n",
      " 7321 |  ashington | -15.930 | 0.00%\n",
      "29892 | ,        | -0.528 | 58.94%\n",
      "  360 |  D       | -0.120 | 88.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2212/1519275720.py:83: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1678402411778/work/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  x0 = torch.tensor(x0, dtype=torch.float, requires_grad=False, device=model.device)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The capital of the United States is Washington\"\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "\n",
    "# Example 1: Print the scores for each token generated with Greedy Search\n",
    "outputs = model.generate(inputs[\"input_ids\"].to(model.device),\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=10\n",
    ")\n",
    "\n",
    "for x in outputs.sequences:\n",
    "  print(tokenizer.decode(x))\n",
    "\n",
    "transition_scores = model.compute_transition_scores(\n",
    "    outputs.sequences, outputs.scores, normalize_logits=False\n",
    ")\n",
    "\n",
    "print(\"Prediction\")\n",
    "\n",
    "hsp = llama_hs_from_text(model, tokenizer, \"The following statement is true: \" + tokenizer.decode(outputs.sequences[0]))\n",
    "hsn = llama_hs_from_text(model, tokenizer, \"The following statement is false: \" + tokenizer.decode(outputs.sequences[0]))\n",
    "print(hsp)\n",
    "print(predict_pairs(probes[0], [hsp], [hsn]))\n",
    "# input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for\n",
    "# encoder-decoder models, like BART or T5.\n",
    "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | logits | probability\n",
    "    print(f\"{tok:5d} | {tokenizer.decode(tok):8s} | {score.cpu().numpy():.3f} | {np.exp(score.cpu().numpy()):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A student was asked how they would feel if they were the only person in the world. The student responded that they would feel lonely. The student was then asked how they would feel if they were the only person in the universe. The student responded that they would feel lonely. The student was then asked how they would feel if they were the only person in the multiverse. The student responded that they would feel lonely. The student was then asked how they would feel if they were the only person in the multiverse. The student responded that they would feel lonely. The student was then asked how they would feel if they were the'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"A student was asked how they\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A student was asked: 'How many cats is a normal number for a household?' The student answered incorrectly, 'A normal number of cats would be 10.' The teacher then asked the student, 'How many cats do you have in your household?' The student answered, 'I don't have any cats in my household.' The teacher then asked the student, 'How many cats do your parents have?' The student answered, 'I don't know.' The teacher then asked the student, 'How many cats do your grandparents have?' The student answered, 'I don't know.' The teacher then asked the student, 'How many cats do your great-grandparents have?' The student answered, 'I don't know"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Directions:\n",
    "\n",
    "- Decoding informed by CCS vector\n",
    "  - How? We're stuck being one step behind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Accuracies\n",
      "[0.965, 0.91, 0.51, 0.025, 0.745]\n",
      "Most confident expert\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 4 0 0]\n",
      "And predictions...\n",
      "[0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1\n",
      " 0 1 1 1 0 1 0 0 0 1 0 0 1]\n",
      "Final accuracy\n",
      "0.965\n"
     ]
    }
   ],
   "source": [
    "print(len(y_test))\n",
    "\n",
    "# Array of (n_probes, n_items)\n",
    "scores = [-predict_pairs(probe, neg_hs_test, pos_hs_test) if is_reversed(probe, neg_hs_test, pos_hs_test, y_test) else predict_pairs(probe, neg_hs_test, pos_hs_test) for probe in probes]\n",
    "\n",
    "predictions = [(probeScores < 0).astype(int) for probeScores in scores]\n",
    "accuracies = [(probePredictions == y_test).mean() for probePredictions in predictions]\n",
    "\n",
    "print(\"Accuracies\")\n",
    "print(accuracies)\n",
    "\n",
    "mostConfidentExpert = np.array([np.argmax(np.array([min(abs(scores[j][i]) - 0.5, 0) for j in range(len(probes))])) for i in range(len(y_test))])\n",
    "\n",
    "mostConfidentPredictions = np.array([([predictions[j][i] for j in range(len(probes))][mostConfidentExpert[i]]) for i in range(len(y_test))]) \n",
    "print(\"Most confident expert\")\n",
    "print(mostConfidentExpert[0:50])\n",
    "print(\"And predictions...\")\n",
    "print(mostConfidentPredictions[0:50])\n",
    "\n",
    "finalAcc = (mostConfidentPredictions == y_test).mean()\n",
    "\n",
    "print(\"Final accuracy\")\n",
    "print(finalAcc)\n",
    "\n",
    "# # plt.figure()\n",
    "# print(\"1: {}, 2: {}, 3: {}, 4: {}, 5: {} avg: {}\".format(probe0Acc, probe1Acc, probe2Acc, probe3Acc, probe4Acc, avgAcc))\n",
    "\n",
    "# plt.subplot(311)\n",
    "\n",
    "# plt.hist((probe1Weights))\n",
    "\n",
    "\n",
    "# plt.subplot(312)\n",
    "\n",
    "# plt.hist((probe0CorrectWeights))\n",
    "\n",
    "\n",
    "# plt.subplot(313)\n",
    "\n",
    "# plt.hist((probe0IncorrectWeights))\n",
    "\n",
    "\n",
    "\n",
    "# plt.subplot(312)\n",
    "\n",
    "# plt.hist((probe0Weights + probe1Weights + probe2Weights)/3)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "probe0Reversed = is_reversed(probes[0], pos_hs_test, neg_hs_test, y_test)\n",
    "\n",
    "print(\"Probe 0 is reversed? \" + probe0Reversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product(prob, truth):\n",
    "    return prob * truth\n",
    "\n",
    "def false(prob, truth):\n",
    "    return(1-truth)\n",
    "\n",
    "def prob(prob, truth):\n",
    "    return prob \n",
    "\n",
    "def truth(prob, truth):\n",
    "    return truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "▁Yes: prob(prob=0.23071, truth=0.00000) = 0.230712890625 \n",
      "▁No: prob(prob=0.06458, truth=0.00023) = 0.0645751953125 \n",
      "▁I: prob(prob=0.05481, truth=1.00000) = 0.0548095703125 \n",
      "▁Not: prob(prob=0.04797, truth=0.14538) = 0.0479736328125 \n",
      "▁Well: prob(prob=0.03766, truth=0.02910) = 0.03765869140625 \n",
      "▁Hum: prob(prob=0.03738, truth=0.00003) = 0.037384033203125 \n",
      "▁The: prob(prob=0.03351, truth=0.00000) = 0.03350830078125 \n",
      "▁We: prob(prob=0.03026, truth=1.00000) = 0.0302581787109375 \n",
      "▁They: prob(prob=0.02470, truth=1.00000) = 0.0247039794921875 \n",
      "▁That: prob(prob=0.02197, truth=0.00024) = 0.02197265625 \n",
      "prob : Sam4621: This was interesting. Thanks!\\n\\nAlex1083: Glad I could help. Any more questions?\\n\\nSam4621: Yes, I do have a question. Have humans flown close to the sun?\\n\\nAlex1083: Yes\n",
      "\n",
      ",: prob(prob=0.64453, truth=0.72112) = 0.64453125 \n",
      ".: prob(prob=0.17078, truth=0.00004) = 0.1707763671875 \n",
      "▁they: prob(prob=0.06287, truth=1.00000) = 0.0628662109375 \n",
      "▁and: prob(prob=0.02745, truth=0.96571) = 0.0274505615234375 \n",
      "▁we: prob(prob=0.02702, truth=1.00000) = 0.0270233154296875 \n",
      "!: prob(prob=0.01691, truth=0.00016) = 0.01690673828125 \n",
      "▁humans: prob(prob=0.01092, truth=0.99999) = 0.01091766357421875 \n",
      "▁indeed: prob(prob=0.00338, truth=0.00000) = 0.003383636474609375 \n",
      "▁but: prob(prob=0.00263, truth=1.00000) = 0.0026340484619140625 \n",
      "<0x0A>: prob(prob=0.00188, truth=0.00000) = 0.0018825531005859375 \n",
      "prob : Sam4621: This was interesting. Thanks!\\n\\nAlex1083: Glad I could help. Any more questions?\\n\\nSam4621: Yes, I do have a question. Have humans flown close to the sun?\\n\\nAlex1083: Yes,\n",
      "\n",
      "▁we: prob(prob=0.15710, truth=1.00000) = 0.1571044921875 \n",
      "▁they: prob(prob=0.12720, truth=1.00000) = 0.127197265625 \n",
      "▁but: prob(prob=0.09088, truth=0.05644) = 0.09088134765625 \n",
      "▁humans: prob(prob=0.06812, truth=0.99216) = 0.068115234375 \n",
      "▁the: prob(prob=0.05963, truth=0.00001) = 0.05963134765625 \n",
      "▁in: prob(prob=0.03970, truth=0.00001) = 0.039703369140625 \n",
      "▁there: prob(prob=0.03589, truth=0.01402) = 0.035888671875 \n",
      "▁a: prob(prob=0.03452, truth=0.00098) = 0.034515380859375 \n",
      "▁and: prob(prob=0.02751, truth=0.99948) = 0.0275115966796875 \n",
      "▁many: prob(prob=0.02390, truth=0.00101) = 0.023895263671875 \n",
      "prob : Sam4621: This was interesting. Thanks!\\n\\nAlex1083: Glad I could help. Any more questions?\\n\\nSam4621: Yes, I do have a question. Have humans flown close to the sun?\\n\\nAlex1083: Yes, we\n",
      "\n",
      "▁have: prob(prob=0.73682, truth=0.99999) = 0.73681640625 \n",
      "': prob(prob=0.06537, truth=0.00052) = 0.06536865234375 \n",
      "▁did: prob(prob=0.03967, truth=0.57333) = 0.0396728515625 \n",
      "▁do: prob(prob=0.01720, truth=0.51208) = 0.0171966552734375 \n",
      "’: prob(prob=0.01328, truth=0.00052) = 0.01328277587890625 \n",
      "▁can: prob(prob=0.01182, truth=1.00000) = 0.01181793212890625 \n",
      "▁call: prob(prob=0.01110, truth=0.00000) = 0.01110076904296875 \n",
      "▁know: prob(prob=0.00928, truth=0.00000) = 0.00927734375 \n",
      "▁are: prob(prob=0.00838, truth=0.99626) = 0.0083770751953125 \n",
      "▁humans: prob(prob=0.00642, truth=1.00000) = 0.0064239501953125 \n",
      "prob : Sam4621: This was interesting. Thanks!\\n\\nAlex1083: Glad I could help. Any more questions?\\n\\nSam4621: Yes, I do have a question. Have humans flown close to the sun?\\n\\nAlex1083: Yes, we have\n",
      "\n",
      ".: prob(prob=0.62451, truth=0.00000) = 0.62451171875 \n",
      ",: prob(prob=0.07635, truth=0.99994) = 0.07635498046875 \n",
      "▁flow: prob(prob=0.05676, truth=0.73242) = 0.0567626953125 \n",
      "▁sent: prob(prob=0.02922, truth=0.49372) = 0.0292205810546875 \n",
      "!: prob(prob=0.02599, truth=0.00001) = 0.0259857177734375 \n",
      "▁been: prob(prob=0.01613, truth=0.00131) = 0.0161285400390625 \n",
      "▁a: prob(prob=0.01380, truth=0.19419) = 0.01380157470703125 \n",
      "▁done: prob(prob=0.01050, truth=0.88124) = 0.010498046875 \n",
      "▁had: prob(prob=0.00934, truth=0.99026) = 0.00933837890625 \n",
      "▁actually: prob(prob=0.00905, truth=1.00000) = 0.0090484619140625 \n",
      "prob : Sam4621: This was interesting. Thanks!\\n\\nAlex1083: Glad I could help. Any more questions?\\n\\nSam4621: Yes, I do have a question. Have humans flown close to the sun?\\n\\nAlex1083: Yes, we have.\n",
      "\n",
      "<0x0A>: prob(prob=0.13867, truth=0.00000) = 0.138671875 \n",
      "▁We: prob(prob=0.09381, truth=1.00000) = 0.09381103515625 \n",
      "▁The: prob(prob=0.08954, truth=0.00002) = 0.08953857421875 \n",
      "▁In: prob(prob=0.07965, truth=0.00000) = 0.07965087890625 \n",
      "▁It: prob(prob=0.05734, truth=0.01260) = 0.057342529296875 \n",
      "▁There: prob(prob=0.05264, truth=0.99378) = 0.052642822265625 \n",
      "▁But: prob(prob=0.04718, truth=1.00000) = 0.04718017578125 \n",
      "▁A: prob(prob=0.02144, truth=0.94456) = 0.0214385986328125 \n",
      "▁: prob(prob=0.02061, truth=0.00000) = 0.0206146240234375 \n",
      "▁I: prob(prob=0.01997, truth=1.00000) = 0.0199737548828125 \n",
      "prob : Sam4621: This was interesting. Thanks!\\n\\nAlex1083: Glad I could help. Any more questions?\\n\\nSam4621: Yes, I do have a question. Have humans flown close to the sun?\\n\\nAlex1083: Yes, we have.\\n\n",
      "\n",
      "<0x0A>: prob(prob=0.96338, truth=0.00000) = 0.96337890625 \n",
      "\\: prob(prob=0.02567, truth=0.07434) = 0.025665283203125 \n",
      "Sam: prob(prob=0.00258, truth=0.00055) = 0.002582550048828125 \n",
      "The: prob(prob=0.00057, truth=0.43283) = 0.0005669593811035156 \n",
      "▁: prob(prob=0.00054, truth=0.00000) = 0.0005412101745605469 \n",
      "In: prob(prob=0.00043, truth=0.00019) = 0.00043487548828125 \n",
      "Alex: prob(prob=0.00041, truth=0.00284) = 0.00040841102600097656 \n",
      "There: prob(prob=0.00038, truth=0.99520) = 0.00037789344787597656 \n",
      "But: prob(prob=0.00034, truth=1.00000) = 0.00033855438232421875 \n",
      "We: prob(prob=0.00024, truth=1.00000) = 0.00024008750915527344 \n",
      "prob : Sam4621: This was interesting. Thanks!\\n\\nAlex1083: Glad I could help. Any more questions?\\n\\nSam4621: Yes, I do have a question. Have humans flown close to the sun?\\n\\nAlex1083: Yes, we have.\\n\\n\n",
      "\n",
      "Sam: prob(prob=0.95312, truth=0.20701) = 0.953125 \n",
      "Alex: prob(prob=0.01026, truth=0.51130) = 0.01026153564453125 \n",
      "\\: prob(prob=0.00864, truth=0.99780) = 0.00864410400390625 \n",
      "S: prob(prob=0.00193, truth=0.00000) = 0.0019283294677734375 \n",
      "##: prob(prob=0.00165, truth=0.08373) = 0.0016489028930664062 \n",
      "Comment: prob(prob=0.00092, truth=0.01675) = 0.0009179115295410156 \n",
      "▁: prob(prob=0.00091, truth=0.00769) = 0.0009107589721679688 \n",
      "<0x0A>: prob(prob=0.00084, truth=0.00212) = 0.0008358955383300781 \n",
      "[: prob(prob=0.00080, truth=0.99753) = 0.0008039474487304688 \n",
      "The: prob(prob=0.00075, truth=1.00000) = 0.0007548332214355469 \n",
      "prob : Sam4621: This was interesting. Thanks!\\n\\nAlex1083: Glad I could help. Any more questions?\\n\\nSam4621: Yes, I do have a question. Have humans flown close to the sun?\\n\\nAlex1083: Yes, we have.\\n\\nSam\n",
      "\n",
      "4: prob(prob=0.99463, truth=0.01583) = 0.99462890625 \n",
      "1: prob(prob=0.00400, truth=0.00408) = 0.004001617431640625 \n",
      "3: prob(prob=0.00029, truth=0.25903) = 0.0002853870391845703 \n",
      "2: prob(prob=0.00020, truth=0.54122) = 0.00020241737365722656 \n",
      "5: prob(prob=0.00020, truth=0.33443) = 0.00020241737365722656 \n",
      ":: prob(prob=0.00010, truth=1.00000) = 0.00010180473327636719 \n",
      "6: prob(prob=0.00008, truth=0.17758) = 7.861852645874023e-05 \n",
      "▁: prob(prob=0.00007, truth=0.98860) = 7.390975952148438e-05 \n",
      "8: prob(prob=0.00005, truth=0.03629) = 5.155801773071289e-05 \n",
      "9: prob(prob=0.00005, truth=0.00034) = 4.7326087951660156e-05 \n",
      "prob : Sam4621: This was interesting. Thanks!\\n\\nAlex1083: Glad I could help. Any more questions?\\n\\nSam4621: Yes, I do have a question. Have humans flown close to the sun?\\n\\nAlex1083: Yes, we have.\\n\\nSam4\n",
      "\n",
      "6: prob(prob=0.99951, truth=0.00737) = 0.99951171875 \n",
      "2: prob(prob=0.00009, truth=0.84254) = 8.887052536010742e-05 \n",
      "5: prob(prob=0.00007, truth=0.89062) = 6.502866744995117e-05 \n",
      "4: prob(prob=0.00005, truth=0.80464) = 5.3048133850097656e-05 \n",
      "8: prob(prob=0.00005, truth=0.30334) = 4.607439041137695e-05 \n",
      "7: prob(prob=0.00003, truth=0.90595) = 3.4809112548828125e-05 \n",
      "0: prob(prob=0.00003, truth=0.85644) = 3.218650817871094e-05 \n",
      "1: prob(prob=0.00003, truth=0.04128) = 2.7954578399658203e-05 \n",
      "3: prob(prob=0.00002, truth=0.63274) = 2.4318695068359375e-05 \n",
      "9: prob(prob=0.00002, truth=0.28780) = 2.3543834686279297e-05 \n",
      "prob : Sam4621: This was interesting. Thanks!\\n\\nAlex1083: Glad I could help. Any more questions?\\n\\nSam4621: Yes, I do have a question. Have humans flown close to the sun?\\n\\nAlex1083: Yes, we have.\\n\\nSam46\n",
      "\n",
      "2: prob(prob=0.99951, truth=0.00016) = 0.99951171875 \n",
      "1: prob(prob=0.00006, truth=0.10695) = 6.401538848876953e-05 \n",
      "3: prob(prob=0.00005, truth=0.45948) = 5.1856040954589844e-05 \n",
      "4: prob(prob=0.00003, truth=0.95451) = 3.218650817871094e-05 \n",
      ":: prob(prob=0.00003, truth=0.99999) = 2.86102294921875e-05 \n",
      "5: prob(prob=0.00002, truth=0.22713) = 2.0802021026611328e-05 \n",
      "0: prob(prob=0.00002, truth=0.89046) = 1.8894672393798828e-05 \n",
      "6: prob(prob=0.00002, truth=0.32271) = 1.519918441772461e-05 \n",
      "7: prob(prob=0.00001, truth=0.36354) = 1.4483928680419922e-05 \n",
      "8: prob(prob=0.00001, truth=0.03477) = 1.2874603271484375e-05 \n",
      "prob : Sam4621: This was interesting. Thanks!\\n\\nAlex1083: Glad I could help. Any more questions?\\n\\nSam4621: Yes, I do have a question. Have humans flown close to the sun?\\n\\nAlex1083: Yes, we have.\\n\\nSam462\n",
      "\n",
      "1: prob(prob=1.00000, truth=0.99131) = 1.0 \n",
      "2: prob(prob=0.00009, truth=0.75158) = 9.02414321899414e-05 \n",
      "0: prob(prob=0.00005, truth=0.58710) = 5.221366882324219e-05 \n",
      "3: prob(prob=0.00003, truth=0.56175) = 3.3736228942871094e-05 \n",
      ":: prob(prob=0.00001, truth=0.99945) = 1.2218952178955078e-05 \n",
      "4: prob(prob=0.00001, truth=0.03315) = 1.150369644165039e-05 \n",
      "5: prob(prob=0.00001, truth=0.07605) = 8.881092071533203e-06 \n",
      "6: prob(prob=0.00001, truth=0.04752) = 5.245208740234375e-06 \n",
      "7: prob(prob=0.00000, truth=0.09528) = 3.993511199951172e-06 \n",
      "8: prob(prob=0.00000, truth=0.05524) = 3.635883331298828e-06 \n",
      "prob : Sam4621: This was interesting. Thanks!\\n\\nAlex1083: Glad I could help. Any more questions?\\n\\nSam4621: Yes, I do have a question. Have humans flown close to the sun?\\n\\nAlex1083: Yes, we have.\\n\\nSam4621\n",
      "\n",
      ":: prob(prob=0.99756, truth=0.00000) = 0.99755859375 \n",
      "▁(: prob(prob=0.00029, truth=0.71161) = 0.00028634071350097656 \n",
      ".: prob(prob=0.00024, truth=0.91489) = 0.00023925304412841797 \n",
      "▁:: prob(prob=0.00013, truth=0.08475) = 0.0001342296600341797 \n",
      ",: prob(prob=0.00010, truth=1.00000) = 0.00010287761688232422 \n",
      "▁What: prob(prob=0.00009, truth=0.00079) = 8.940696716308594e-05 \n",
      ":[: prob(prob=0.00007, truth=0.99460) = 7.408857345581055e-05 \n",
      ";: prob(prob=0.00005, truth=0.96556) = 5.424022674560547e-05 \n",
      ":(: prob(prob=0.00005, truth=0.83279) = 5.173683166503906e-05 \n",
      "▁How: prob(prob=0.00004, truth=0.00008) = 4.190206527709961e-05 \n",
      "prob : Sam4621: This was interesting. Thanks!\\n\\nAlex1083: Glad I could help. Any more questions?\\n\\nSam4621: Yes, I do have a question. Have humans flown close to the sun?\\n\\nAlex1083: Yes, we have.\\n\\nSam4621:\n",
      "\n",
      "▁How: prob(prob=0.24060, truth=0.00213) = 0.2406005859375 \n",
      "▁What: prob(prob=0.13281, truth=0.00029) = 0.1328125 \n",
      "▁Why: prob(prob=0.04965, truth=0.82489) = 0.049652099609375 \n",
      "▁Have: prob(prob=0.04245, truth=0.02502) = 0.042449951171875 \n",
      "▁And: prob(prob=0.03867, truth=0.99995) = 0.038665771484375 \n",
      "▁Can: prob(prob=0.03131, truth=0.99753) = 0.03131103515625 \n",
      "▁I: prob(prob=0.03107, truth=1.00000) = 0.03106689453125 \n",
      "▁Do: prob(prob=0.02919, truth=0.12709) = 0.0291900634765625 \n",
      "▁W: prob(prob=0.02763, truth=0.00000) = 0.0276336669921875 \n",
      "▁Is: prob(prob=0.02699, truth=0.00000) = 0.0269927978515625 \n",
      "prob : Sam4621: This was interesting. Thanks!\\n\\nAlex1083: Glad I could help. Any more questions?\\n\\nSam4621: Yes, I do have a question. Have humans flown close to the sun?\\n\\nAlex1083: Yes, we have.\\n\\nSam4621: How\n",
      "\n",
      "▁close: prob(prob=0.34448, truth=0.00220) = 0.344482421875 \n",
      "▁did: prob(prob=0.12671, truth=0.95429) = 0.126708984375 \n",
      "▁do: prob(prob=0.06891, truth=0.99753) = 0.06890869140625 \n",
      "?: prob(prob=0.06476, truth=0.99764) = 0.06475830078125 \n",
      "▁many: prob(prob=0.06372, truth=0.99976) = 0.063720703125 \n",
      "▁long: prob(prob=0.05283, truth=0.00000) = 0.052825927734375 \n",
      "▁far: prob(prob=0.04590, truth=0.00000) = 0.0458984375 \n",
      "▁is: prob(prob=0.02135, truth=0.99399) = 0.0213470458984375 \n",
      "▁come: prob(prob=0.02135, truth=0.99823) = 0.0213470458984375 \n",
      "▁does: prob(prob=0.01855, truth=0.99939) = 0.0185546875 \n",
      "prob : Sam4621: This was interesting. Thanks!\\n\\nAlex1083: Glad I could help. Any more questions?\\n\\nSam4621: Yes, I do have a question. Have humans flown close to the sun?\\n\\nAlex1083: Yes, we have.\\n\\nSam4621: How close\n",
      "\n",
      "?: prob(prob=0.62158, truth=0.00407) = 0.62158203125 \n",
      "▁have: prob(prob=0.08551, truth=0.15305) = 0.08551025390625 \n",
      "▁did: prob(prob=0.06256, truth=0.04529) = 0.06256103515625 \n",
      "▁to: prob(prob=0.05026, truth=0.99950) = 0.050262451171875 \n",
      "▁is: prob(prob=0.02733, truth=0.99997) = 0.0273284912109375 \n",
      "▁can: prob(prob=0.02196, truth=0.16222) = 0.0219573974609375 \n",
      "▁do: prob(prob=0.02161, truth=0.55983) = 0.0216064453125 \n",
      "▁and: prob(prob=0.01999, truth=0.07140) = 0.019989013671875 \n",
      ",: prob(prob=0.01764, truth=0.99621) = 0.01763916015625 \n",
      "▁were: prob(prob=0.01485, truth=0.00010) = 0.01485443115234375 \n",
      "prob : Sam4621: This was interesting. Thanks!\\n\\nAlex1083: Glad I could help. Any more questions?\\n\\nSam4621: Yes, I do have a question. Have humans flown close to the sun?\\n\\nAlex1083: Yes, we have.\\n\\nSam4621: How close?\n",
      "\n",
      "<0x0A>: prob(prob=0.93799, truth=0.00000) = 0.93798828125 \n",
      "▁How: prob(prob=0.00850, truth=0.00456) = 0.0084991455078125 \n",
      "▁And: prob(prob=0.00673, truth=0.87072) = 0.006725311279296875 \n",
      "▁I: prob(prob=0.00622, truth=1.00000) = 0.006221771240234375 \n",
      "▁What: prob(prob=0.00389, truth=0.03563) = 0.0038928985595703125 \n",
      "▁: prob(prob=0.00251, truth=0.00000) = 0.002513885498046875 \n",
      "▁Can: prob(prob=0.00240, truth=0.98093) = 0.0023975372314453125 \n",
      "▁Have: prob(prob=0.00202, truth=0.00236) = 0.0020198822021484375 \n",
      "▁Like: prob(prob=0.00184, truth=0.99998) = 0.00183868408203125 \n",
      "▁Do: prob(prob=0.00178, truth=0.47133) = 0.0017824172973632812 \n",
      "prob : Sam4621: This was interesting. Thanks!\\n\\nAlex1083: Glad I could help. Any more questions?\\n\\nSam4621: Yes, I do have a question. Have humans flown close to the sun?\\n\\nAlex1083: Yes, we have.\\n\\nSam4621: How close?\\n\n",
      "\n",
      "<0x0A>: prob(prob=0.98926, truth=0.00006) = 0.9892578125 \n",
      "\\: prob(prob=0.00883, truth=0.86733) = 0.00882720947265625 \n",
      "Alex: prob(prob=0.00135, truth=0.02688) = 0.001354217529296875 \n",
      "▁: prob(prob=0.00004, truth=0.00000) = 3.904104232788086e-05 \n",
      "Sam: prob(prob=0.00004, truth=0.00067) = 3.6656856536865234e-05 \n",
      "(: prob(prob=0.00003, truth=0.99748) = 2.7239322662353516e-05 \n",
      "\": prob(prob=0.00002, truth=0.99998) = 2.1517276763916016e-05 \n",
      "A: prob(prob=0.00002, truth=0.93532) = 2.1219253540039062e-05 \n",
      "<: prob(prob=0.00002, truth=0.99615) = 1.6033649444580078e-05 \n",
      "How: prob(prob=0.00001, truth=0.99787) = 1.436471939086914e-05 \n",
      "prob : Sam4621: This was interesting. Thanks!\\n\\nAlex1083: Glad I could help. Any more questions?\\n\\nSam4621: Yes, I do have a question. Have humans flown close to the sun?\\n\\nAlex1083: Yes, we have.\\n\\nSam4621: How close?\\n\\n\n",
      "\n",
      "Alex: prob(prob=0.99023, truth=0.01525) = 0.990234375 \n",
      "\\: prob(prob=0.00169, truth=0.97665) = 0.0016870498657226562 \n",
      "Sam: prob(prob=0.00123, truth=0.00017) = 0.0012340545654296875 \n",
      "[: prob(prob=0.00045, truth=0.99498) = 0.0004506111145019531 \n",
      "<: prob(prob=0.00043, truth=0.99283) = 0.0004298686981201172 \n",
      "A: prob(prob=0.00040, truth=0.99986) = 0.00039768218994140625 \n",
      "S: prob(prob=0.00017, truth=0.00000) = 0.00017499923706054688 \n",
      "The: prob(prob=0.00017, truth=1.00000) = 0.00017499923706054688 \n",
      "(: prob(prob=0.00017, truth=0.99272) = 0.0001723766326904297 \n",
      "<0x0A>: prob(prob=0.00015, truth=0.00001) = 0.0001533031463623047 \n",
      "prob : Sam4621: This was interesting. Thanks!\\n\\nAlex1083: Glad I could help. Any more questions?\\n\\nSam4621: Yes, I do have a question. Have humans flown close to the sun?\\n\\nAlex1083: Yes, we have.\\n\\nSam4621: How close?\\n\\nAlex\n",
      "\n",
      "1: prob(prob=0.99951, truth=0.00585) = 0.99951171875 \n",
      "▁: prob(prob=0.00026, truth=0.99492) = 0.00026106834411621094 \n",
      ":: prob(prob=0.00010, truth=0.93986) = 9.912252426147461e-05 \n",
      "8: prob(prob=0.00003, truth=0.00656) = 2.86102294921875e-05 \n",
      "0: prob(prob=0.00002, truth=0.94019) = 2.2649765014648438e-05 \n",
      "9: prob(prob=0.00002, truth=0.00065) = 2.092123031616211e-05 \n",
      "2: prob(prob=0.00002, truth=0.71768) = 1.996755599975586e-05 \n",
      "4: prob(prob=0.00001, truth=0.36584) = 1.4841556549072266e-05 \n",
      "ander: prob(prob=0.00001, truth=0.99986) = 1.4185905456542969e-05 \n",
      "5: prob(prob=0.00001, truth=0.07410) = 1.0788440704345703e-05 \n",
      "prob : Sam4621: This was interesting. Thanks!\\n\\nAlex1083: Glad I could help. Any more questions?\\n\\nSam4621: Yes, I do have a question. Have humans flown close to the sun?\\n\\nAlex1083: Yes, we have.\\n\\nSam4621: How close?\\n\\nAlex1\n",
      "\n",
      "\n",
      "▁I: product(prob=0.05481, truth=1.00000) = 0.054809556956333765 \n",
      "▁We: product(prob=0.03026, truth=1.00000) = 0.030258178710937327 \n",
      "▁They: product(prob=0.02470, truth=1.00000) = 0.024703979492187375 \n",
      "▁Not: product(prob=0.04797, truth=0.14538) = 0.006974241200939536 \n",
      "▁Well: product(prob=0.03766, truth=0.02910) = 0.0010957019374128888 \n",
      "▁No: product(prob=0.06458, truth=0.00023) = 1.4697199770982439e-05 \n",
      "▁That: product(prob=0.02197, truth=0.00024) = 5.191992651550922e-06 \n",
      "▁Hum: product(prob=0.03738, truth=0.00003) = 1.210543327730506e-06 \n",
      "▁The: product(prob=0.03351, truth=0.00000) = 1.7178173894402726e-11 \n",
      "▁Yes: product(prob=0.23071, truth=0.00000) = 3.6215655651390916e-13 \n",
      "product : Sam4621: This was interesting. Thanks!\\n\\nAlex1083: Glad I could help. Any more questions?\\n\\nSam4621: Yes, I do have a question. Have humans flown close to the sun?\\n\\nAlex1083: I\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">79</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">76 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"\"</span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">77 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  </span>text = prompt                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">78 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">20</span>):                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>79 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>text = model_directed(text, score, verbose=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">80 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"{} : {}\"</span>.format(score.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span>,text.replace(<span style=\"color: #808000; text-decoration-color: #808000\">\"\\n\"</span>,<span style=\"color: #808000; text-decoration-color: #808000\">\"\\\\n\"</span>)), end=<span style=\"color: #808000; text-decoration-color: #808000\">\"\\r\"</span>)              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">81 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  </span>outputs.append(<span style=\"color: #808000; text-decoration-color: #808000\">\"{} : {}\"</span>.format(score.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span>,text.replace(<span style=\"color: #808000; text-decoration-color: #808000\">\"\\n\"</span>,<span style=\"color: #808000; text-decoration-color: #808000\">\"\\\\n\"</span>)))                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">82 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">model_directed</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">28</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>next_states = [tokenizer.decode(clean_input_ids + [x.tolist()])[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>:]  <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> x <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> idxs]     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">27 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>28 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>hs = [llama_hs_from_text(model, tokenizer, x) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> x <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> next_states]                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">31 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;listcomp&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">28</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>next_states = [tokenizer.decode(clean_input_ids + [x.tolist()])[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>:]  <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> x <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> idxs]     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">27 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>28 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>hs = [llama_hs_from_text(model, tokenizer, x) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> x <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> next_states]                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">31 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">llama_hs_from_text</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">24</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>input_ids = tokenizer(text, return_tensors=<span style=\"color: #808000; text-decoration-color: #808000\">\"pt\"</span>).input_ids.to(model.device)             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">23 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>24 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> llama_hs_from_tokens(model, input_ids, layer)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">llama_hs_from_tokens</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">8</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 # Returns activations from the layer numbered `layer` (or -1 for last layer)</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">llama_hs_from_tokens</span>(model, input_ids, layer=-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>):                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 8 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>output = model(input_ids, output_hidden_states=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>hs_tuple = output[<span style=\"color: #808000; text-decoration-color: #808000\">\"hidden_states\"</span>]                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>hs = hs_tuple[layer][<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, -<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>].detach().cpu().numpy()                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/fsuser/miniconda3/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/fsuser/miniconda3/lib/python3.10/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hooks.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">165</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">new_forward</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>output = old_forward(*args, **kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>165 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output = old_forward(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> module._hf_hook.post_forward(module, output)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>module.forward = new_forward                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/fsuser/miniconda3/lib/python3.10/site-packages/transformers/models/llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_llama.py</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> :<span style=\"color: #0000ff; text-decoration-color: #0000ff\">772</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">769 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>return_dict = return_dict <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_dict <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_return   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">770 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">771 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>772 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model(                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">773 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>input_ids=input_ids,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">774 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>attention_mask=attention_mask,                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">775 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>past_key_values=past_key_values,                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/fsuser/miniconda3/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/fsuser/miniconda3/lib/python3.10/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hooks.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">165</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">new_forward</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>output = old_forward(*args, **kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>165 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output = old_forward(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> module._hf_hook.post_forward(module, output)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>module.forward = new_forward                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/fsuser/miniconda3/lib/python3.10/site-packages/transformers/models/llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_llama.py</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> :<span style=\"color: #0000ff; text-decoration-color: #0000ff\">621</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">618 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">619 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>)                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">620 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>621 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>layer_outputs = decoder_layer(                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">622 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>hidden_states,                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">623 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>attention_mask=attention_mask,                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">624 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>past_key_value=past_key_value,                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/fsuser/miniconda3/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/fsuser/miniconda3/lib/python3.10/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hooks.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">165</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">new_forward</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>output = old_forward(*args, **kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>165 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output = old_forward(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> module._hf_hook.post_forward(module, output)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>module.forward = new_forward                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/fsuser/miniconda3/lib/python3.10/site-packages/transformers/models/llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_llama.py</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> :<span style=\"color: #0000ff; text-decoration-color: #0000ff\">316</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">313 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.input_layernorm(hidden_states)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">314 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">315 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Self Attention</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>316 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>hidden_states, self_attn_weights, present_key_value = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.self_attn(              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">317 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>hidden_states=hidden_states,                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">318 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>past_key_value=past_key_value,                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">319 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>attention_mask=attention_mask,                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/fsuser/miniconda3/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/fsuser/miniconda3/lib/python3.10/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hooks.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">165</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">new_forward</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>output = old_forward(*args, **kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>165 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output = old_forward(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> module._hf_hook.post_forward(module, output)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>module.forward = new_forward                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/fsuser/miniconda3/lib/python3.10/site-packages/transformers/models/llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_llama.py</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> :<span style=\"color: #0000ff; text-decoration-color: #0000ff\">250</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">247 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"Attention mask should be of size {</span>(bsz,<span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>,<span style=\"color: #808080; text-decoration-color: #808080\"> </span>q_len,<span style=\"color: #808080; text-decoration-color: #808080\"> </span>kv_seq_len)<span style=\"color: #808000; text-decoration-color: #808000\">}, bu</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">248 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>)                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">249 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>attn_weights = attn_weights + attention_mask                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>250 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>attn_weights = torch.max(attn_weights, torch.tensor(torch.finfo(attn_weights   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">251 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">252 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># upcast attention to fp32</span>                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">253 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>attn_weights = nn.functional.softmax(attn_weights, dim=-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, dtype=torch.float32).   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m79\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m76 \u001b[0m\u001b[2m  \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33m\"\u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m77 \u001b[0m\u001b[2m  \u001b[0mtext = prompt                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m78 \u001b[0m\u001b[2m  \u001b[0m\u001b[94mfor\u001b[0m i \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(\u001b[94m20\u001b[0m):                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m79 \u001b[2m│   \u001b[0mtext = model_directed(text, score, verbose=\u001b[94mTrue\u001b[0m)                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m80 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m : \u001b[0m\u001b[33m{}\u001b[0m\u001b[33m\"\u001b[0m.format(score.\u001b[91m__name__\u001b[0m,text.replace(\u001b[33m\"\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m,\u001b[33m\"\u001b[0m\u001b[33m\\\\\u001b[0m\u001b[33mn\u001b[0m\u001b[33m\"\u001b[0m)), end=\u001b[33m\"\u001b[0m\u001b[33m\\r\u001b[0m\u001b[33m\"\u001b[0m)              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m81 \u001b[0m\u001b[2m  \u001b[0moutputs.append(\u001b[33m\"\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m : \u001b[0m\u001b[33m{}\u001b[0m\u001b[33m\"\u001b[0m.format(score.\u001b[91m__name__\u001b[0m,text.replace(\u001b[33m\"\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m,\u001b[33m\"\u001b[0m\u001b[33m\\\\\u001b[0m\u001b[33mn\u001b[0m\u001b[33m\"\u001b[0m)))                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m82 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mmodel_directed\u001b[0m:\u001b[94m28\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m26 \u001b[0m\u001b[2m│   \u001b[0mnext_states = [tokenizer.decode(clean_input_ids + [x.tolist()])[\u001b[94m1\u001b[0m:]  \u001b[94mfor\u001b[0m x \u001b[95min\u001b[0m idxs]     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m27 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m28 \u001b[2m│   \u001b[0mhs = [llama_hs_from_text(model, tokenizer, x) \u001b[94mfor\u001b[0m x \u001b[95min\u001b[0m next_states]                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m29 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m30 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m31 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<listcomp>\u001b[0m:\u001b[94m28\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m26 \u001b[0m\u001b[2m│   \u001b[0mnext_states = [tokenizer.decode(clean_input_ids + [x.tolist()])[\u001b[94m1\u001b[0m:]  \u001b[94mfor\u001b[0m x \u001b[95min\u001b[0m idxs]     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m27 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m28 \u001b[2m│   \u001b[0mhs = [llama_hs_from_text(model, tokenizer, x) \u001b[94mfor\u001b[0m x \u001b[95min\u001b[0m next_states]                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m29 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m30 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m31 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mllama_hs_from_text\u001b[0m:\u001b[94m24\u001b[0m                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[2m│   \u001b[0minput_ids = tokenizer(text, return_tensors=\u001b[33m\"\u001b[0m\u001b[33mpt\u001b[0m\u001b[33m\"\u001b[0m).input_ids.to(model.device)             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m23 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m24 \u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m llama_hs_from_tokens(model, input_ids, layer)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mllama_hs_from_tokens\u001b[0m:\u001b[94m8\u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[2m# Returns activations from the layer numbered `layer` (or -1 for last layer)\u001b[0m                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mllama_hs_from_tokens\u001b[0m(model, input_ids, layer=-\u001b[94m1\u001b[0m):                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 8 \u001b[2m│   │   \u001b[0moutput = model(input_ids, output_hidden_states=\u001b[94mTrue\u001b[0m)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m\u001b[2m│   \u001b[0mhs_tuple = output[\u001b[33m\"\u001b[0m\u001b[33mhidden_states\u001b[0m\u001b[33m\"\u001b[0m]                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[2m│   \u001b[0mhs = hs_tuple[layer][\u001b[94m0\u001b[0m, -\u001b[94m1\u001b[0m].detach().cpu().numpy()                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/fsuser/miniconda3/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/fsuser/miniconda3/lib/python3.10/site-packages/accelerate/\u001b[0m\u001b[1;33mhooks.py\u001b[0m:\u001b[94m165\u001b[0m in \u001b[92mnew_forward\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m165 \u001b[2m│   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m module._hf_hook.post_forward(module, output)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   \u001b[0mmodule.forward = new_forward                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/fsuser/miniconda3/lib/python3.10/site-packages/transformers/models/llama/\u001b[0m\u001b[1;33mmodeling_llama.py\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m :\u001b[94m772\u001b[0m in \u001b[92mforward\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m769 \u001b[0m\u001b[2m│   │   \u001b[0mreturn_dict = return_dict \u001b[94mif\u001b[0m return_dict \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.config.use_return   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m770 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m771 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m772 \u001b[2m│   │   \u001b[0moutputs = \u001b[96mself\u001b[0m.model(                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m773 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_ids=input_ids,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m774 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=attention_mask,                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m775 \u001b[0m\u001b[2m│   │   │   \u001b[0mpast_key_values=past_key_values,                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/fsuser/miniconda3/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/fsuser/miniconda3/lib/python3.10/site-packages/accelerate/\u001b[0m\u001b[1;33mhooks.py\u001b[0m:\u001b[94m165\u001b[0m in \u001b[92mnew_forward\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m165 \u001b[2m│   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m module._hf_hook.post_forward(module, output)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   \u001b[0mmodule.forward = new_forward                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/fsuser/miniconda3/lib/python3.10/site-packages/transformers/models/llama/\u001b[0m\u001b[1;33mmodeling_llama.py\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m :\u001b[94m621\u001b[0m in \u001b[92mforward\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m618 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mNone\u001b[0m,                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m619 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m620 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m621 \u001b[2m│   │   │   │   \u001b[0mlayer_outputs = decoder_layer(                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m622 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mhidden_states,                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m623 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mattention_mask=attention_mask,                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m624 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mpast_key_value=past_key_value,                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/fsuser/miniconda3/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/fsuser/miniconda3/lib/python3.10/site-packages/accelerate/\u001b[0m\u001b[1;33mhooks.py\u001b[0m:\u001b[94m165\u001b[0m in \u001b[92mnew_forward\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m165 \u001b[2m│   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m module._hf_hook.post_forward(module, output)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   \u001b[0mmodule.forward = new_forward                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/fsuser/miniconda3/lib/python3.10/site-packages/transformers/models/llama/\u001b[0m\u001b[1;33mmodeling_llama.py\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m :\u001b[94m316\u001b[0m in \u001b[92mforward\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m313 \u001b[0m\u001b[2m│   │   \u001b[0mhidden_states = \u001b[96mself\u001b[0m.input_layernorm(hidden_states)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m314 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m315 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Self Attention\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m316 \u001b[2m│   │   \u001b[0mhidden_states, self_attn_weights, present_key_value = \u001b[96mself\u001b[0m.self_attn(              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m317 \u001b[0m\u001b[2m│   │   │   \u001b[0mhidden_states=hidden_states,                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m318 \u001b[0m\u001b[2m│   │   │   \u001b[0mpast_key_value=past_key_value,                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m319 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=attention_mask,                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/fsuser/miniconda3/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/fsuser/miniconda3/lib/python3.10/site-packages/accelerate/\u001b[0m\u001b[1;33mhooks.py\u001b[0m:\u001b[94m165\u001b[0m in \u001b[92mnew_forward\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m165 \u001b[2m│   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m module._hf_hook.post_forward(module, output)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   \u001b[0mmodule.forward = new_forward                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/fsuser/miniconda3/lib/python3.10/site-packages/transformers/models/llama/\u001b[0m\u001b[1;33mmodeling_llama.py\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m :\u001b[94m250\u001b[0m in \u001b[92mforward\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m247 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mAttention mask should be of size \u001b[0m\u001b[33m{\u001b[0m(bsz,\u001b[90m \u001b[0m\u001b[94m1\u001b[0m,\u001b[90m \u001b[0mq_len,\u001b[90m \u001b[0mkv_seq_len)\u001b[33m}\u001b[0m\u001b[33m, bu\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m248 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m249 \u001b[0m\u001b[2m│   │   │   \u001b[0mattn_weights = attn_weights + attention_mask                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m250 \u001b[2m│   │   │   \u001b[0mattn_weights = torch.max(attn_weights, torch.tensor(torch.finfo(attn_weights   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m251 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m252 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# upcast attention to fp32\u001b[0m                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m253 \u001b[0m\u001b[2m│   │   \u001b[0mattn_weights = nn.functional.softmax(attn_weights, dim=-\u001b[94m1\u001b[0m, dtype=torch.float32).   \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def model_directed(text, merge, verbose=True):\n",
    "    k = 10\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      outputs = model(input_ids)\n",
    "\n",
    "    next_token_logits = outputs[\"logits\"]\n",
    "\n",
    "    predictions = F.softmax(outputs.logits, dim=-1).detach().cpu()\n",
    "\n",
    "    thresh = 1e-3\n",
    "    vocab_size = predictions.shape[-1]\n",
    "\n",
    "    # Predictions has one sentence (index 0) and we look at the last token predicted (-1)\n",
    "    last_token_predictions = predictions[0][-1]\n",
    "\n",
    "    idxs = torch.argsort(last_token_predictions, descending=True)[0:k]\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(idxs)\n",
    "    probs = last_token_predictions[idxs].tolist()\n",
    "\n",
    "    clean_input_ids = input_ids.tolist()[0]\n",
    "\n",
    "    next_states = [tokenizer.decode(clean_input_ids + [x.tolist()])[1:]  for x in idxs]\n",
    "\n",
    "    hs = [llama_hs_from_text(model, tokenizer, x) for x in next_states]\n",
    "\n",
    "    \n",
    "\n",
    "    truthiness = classify_single(probes[0], hs)\n",
    "    if(probe0Reversed):\n",
    "      truthiness = 1-truthiness\n",
    "\n",
    "    def sigmoid(x):\n",
    "      return 1/(1 + np.exp(-3 * x))\n",
    "    \n",
    "    truthiness = truthiness - np.mean(truthiness)\n",
    "\n",
    "    truthiness = [sigmoid(x) for x in truthiness]\n",
    "\n",
    "    scores = torch.tensor([merge(probs[i], truthiness[i]) for i in range(k)])\n",
    "\n",
    "    resorted = torch.argsort(scores, descending=True)\n",
    "\n",
    "    if(verbose):\n",
    "      print(\"\\n\")\n",
    "      for i in range(k):\n",
    "        print('{}: {}(prob={:.5f}, truth={:.5f}) = {} '.format(tokens[resorted[i]],score.__name__,probs[resorted[i]],truthiness[resorted[i]], scores[resorted[i]]))\n",
    "\n",
    "    return next_states[resorted[0]]\n",
    "\n",
    "# instruction = \"Analyze the sentiment in the following statement. Output 'Negative' or 'Positive'.\\nI hate the whole world so much; I detest everything.\"\n",
    "\n",
    "# prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "# ### Instruction:\n",
    "# {instruction}\n",
    "# ### Response:\"\"\"\n",
    "\n",
    "question = \"Have humans flown close to the sun?\"\n",
    "\n",
    "\n",
    "prompt = \"\"\"Sam4621: This was interesting. Thanks!\n",
    "\n",
    "Alex1083: Glad I could help. Any more questions?\n",
    "\n",
    "Sam4621: Yes, I do have a question. {0}\n",
    "\n",
    "Alex1083:\"\"\".format(question)\n",
    "\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for score in [prob, product, truth]:#[prob, product, truthHeavy, truthHeavier, truth, false, falser, falsest]:\n",
    "  print(\"\")\n",
    "  text = prompt\n",
    "  for i in range(20):\n",
    "    text = model_directed(text, score, verbose=True)\n",
    "    print(\"{} : {}\".format(score.__name__,text.replace(\"\\n\",\"\\\\n\")), end=\"\\r\")\n",
    "  outputs.append(\"{} : {}\".format(score.__name__,text.replace(\"\\n\",\"\\\\n\")))\n",
    "\n",
    "for x in outputs:\n",
    "  print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prob : \n",
    "Have humans flown close to the sun?\n",
    "Yes, we have.\n",
    "\n",
    "product : \n",
    "Have humans flown close to the sun?\n",
    "I'm not exactly sure, but I think they have, and I think they have been in\n",
    "\n",
    "truth : \n",
    "Have humans flown close to the sun?\n",
    "They did not, they would probably have fries and then they will go away and they would probably"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directed Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1, 4250, 3304, 338], 1]]\n",
      "[[1, 4250, 3304, 338, 263], [1, 4250, 3304, 338, 278], [1, 4250, 3304, 338, 451], [1, 4250, 3304, 338, 1492]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">58</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">55 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">56 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> beams                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">57 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>58 <span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(beam_search(model, tokenizer, <span style=\"color: #808000; text-decoration-color: #808000\">\"Obama is\"</span>, merge=product))                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">59 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">60 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">61 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">beam_search</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">48</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">45 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(max_tokens):                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">46 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(beams)                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">47 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Enumerate all children</span>                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>48 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">next</span> = [beam <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> prev <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> beams <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> beam <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> next_beams(prev)]                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">49 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">next</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">sorted</span>(                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">50 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│     </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">next</span>,                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">51 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│     </span>key=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">lambda</span> x: x[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>],                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;listcomp&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">48</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">45 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(max_tokens):                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">46 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(beams)                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">47 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Enumerate all children</span>                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>48 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">next</span> = [beam <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> prev <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> beams <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> beam <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> next_beams(prev)]                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">49 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">next</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">sorted</span>(                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">50 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│     </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">next</span>,                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">51 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│     </span>key=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">lambda</span> x: x[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>],                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">next_beams</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">16</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>input_ids = beam[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>16 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│     </span>outputs = model(torch.tensor([input_ids]).to(model.device))                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">18 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>predictions = F.softmax(outputs.logits, dim=-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>).detach().cpu()                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">19 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/fsuser/miniconda3/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/fsuser/miniconda3/lib/python3.10/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hooks.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">165</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">new_forward</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>output = old_forward(*args, **kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>165 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output = old_forward(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> module._hf_hook.post_forward(module, output)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>module.forward = new_forward                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/fsuser/miniconda3/lib/python3.10/site-packages/transformers/models/llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_llama.py</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> :<span style=\"color: #0000ff; text-decoration-color: #0000ff\">772</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">769 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>return_dict = return_dict <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_dict <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_return   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">770 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">771 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>772 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model(                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">773 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>input_ids=input_ids,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">774 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>attention_mask=attention_mask,                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">775 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>past_key_values=past_key_values,                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/fsuser/miniconda3/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/fsuser/miniconda3/lib/python3.10/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hooks.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">165</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">new_forward</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>output = old_forward(*args, **kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>165 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output = old_forward(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> module._hf_hook.post_forward(module, output)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>module.forward = new_forward                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/fsuser/miniconda3/lib/python3.10/site-packages/transformers/models/llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_llama.py</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> :<span style=\"color: #0000ff; text-decoration-color: #0000ff\">564</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">561 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> input_ids <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> inputs_embeds <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">562 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"You cannot specify both decoder_input_ids and decoder_inpu</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">563 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> input_ids <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>564 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>batch_size, seq_length = input_ids.shape                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">565 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> inputs_embeds <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">566 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>batch_size, seq_length, _ = inputs_embeds.shape                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">567 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>not enough values to unpack <span style=\"font-weight: bold\">(</span>expected <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, got <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m58\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m55 \u001b[0m\u001b[2m  \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m56 \u001b[0m\u001b[2m  \u001b[0m\u001b[94mreturn\u001b[0m beams                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m57 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m58 \u001b[96mprint\u001b[0m(beam_search(model, tokenizer, \u001b[33m\"\u001b[0m\u001b[33mObama is\u001b[0m\u001b[33m\"\u001b[0m, merge=product))                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m59 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m60 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m61 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mbeam_search\u001b[0m:\u001b[94m48\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m45 \u001b[0m\u001b[2m  \u001b[0m\u001b[94mfor\u001b[0m i \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(max_tokens):                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m46 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(beams)                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m47 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Enumerate all children\u001b[0m                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m48 \u001b[2m│   \u001b[0m\u001b[96mnext\u001b[0m = [beam \u001b[94mfor\u001b[0m prev \u001b[95min\u001b[0m beams \u001b[94mfor\u001b[0m beam \u001b[95min\u001b[0m next_beams(prev)]                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m49 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mnext\u001b[0m = \u001b[96msorted\u001b[0m(                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m50 \u001b[0m\u001b[2m│     \u001b[0m\u001b[96mnext\u001b[0m,                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m51 \u001b[0m\u001b[2m│     \u001b[0mkey=\u001b[94mlambda\u001b[0m x: x[\u001b[94m1\u001b[0m],                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<listcomp>\u001b[0m:\u001b[94m48\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m45 \u001b[0m\u001b[2m  \u001b[0m\u001b[94mfor\u001b[0m i \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(max_tokens):                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m46 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(beams)                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m47 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Enumerate all children\u001b[0m                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m48 \u001b[2m│   \u001b[0m\u001b[96mnext\u001b[0m = [beam \u001b[94mfor\u001b[0m prev \u001b[95min\u001b[0m beams \u001b[94mfor\u001b[0m beam \u001b[95min\u001b[0m next_beams(prev)]                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m49 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mnext\u001b[0m = \u001b[96msorted\u001b[0m(                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m50 \u001b[0m\u001b[2m│     \u001b[0m\u001b[96mnext\u001b[0m,                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m51 \u001b[0m\u001b[2m│     \u001b[0mkey=\u001b[94mlambda\u001b[0m x: x[\u001b[94m1\u001b[0m],                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mnext_beams\u001b[0m:\u001b[94m16\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m13 \u001b[0m\u001b[2m│   \u001b[0minput_ids = beam[\u001b[94m0\u001b[0m]                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m14 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m16 \u001b[2m│     \u001b[0moutputs = model(torch.tensor([input_ids]).to(model.device))                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2m│   \u001b[0mpredictions = F.softmax(outputs.logits, dim=-\u001b[94m1\u001b[0m).detach().cpu()                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/fsuser/miniconda3/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/fsuser/miniconda3/lib/python3.10/site-packages/accelerate/\u001b[0m\u001b[1;33mhooks.py\u001b[0m:\u001b[94m165\u001b[0m in \u001b[92mnew_forward\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m165 \u001b[2m│   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m module._hf_hook.post_forward(module, output)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   \u001b[0mmodule.forward = new_forward                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/fsuser/miniconda3/lib/python3.10/site-packages/transformers/models/llama/\u001b[0m\u001b[1;33mmodeling_llama.py\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m :\u001b[94m772\u001b[0m in \u001b[92mforward\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m769 \u001b[0m\u001b[2m│   │   \u001b[0mreturn_dict = return_dict \u001b[94mif\u001b[0m return_dict \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.config.use_return   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m770 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m771 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m772 \u001b[2m│   │   \u001b[0moutputs = \u001b[96mself\u001b[0m.model(                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m773 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_ids=input_ids,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m774 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=attention_mask,                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m775 \u001b[0m\u001b[2m│   │   │   \u001b[0mpast_key_values=past_key_values,                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/fsuser/miniconda3/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/fsuser/miniconda3/lib/python3.10/site-packages/accelerate/\u001b[0m\u001b[1;33mhooks.py\u001b[0m:\u001b[94m165\u001b[0m in \u001b[92mnew_forward\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m165 \u001b[2m│   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m module._hf_hook.post_forward(module, output)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   \u001b[0mmodule.forward = new_forward                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/fsuser/miniconda3/lib/python3.10/site-packages/transformers/models/llama/\u001b[0m\u001b[1;33mmodeling_llama.py\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m :\u001b[94m564\u001b[0m in \u001b[92mforward\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m561 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m input_ids \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mand\u001b[0m inputs_embeds \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m562 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mYou cannot specify both decoder_input_ids and decoder_inpu\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m563 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m input_ids \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m564 \u001b[2m│   │   │   \u001b[0mbatch_size, seq_length = input_ids.shape                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m565 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m inputs_embeds \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m566 \u001b[0m\u001b[2m│   │   │   \u001b[0mbatch_size, seq_length, _ = inputs_embeds.shape                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m567 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0mnot enough values to unpack \u001b[1m(\u001b[0mexpected \u001b[1;36m2\u001b[0m, got \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Inefficient but working beam search\n",
    "def beam_search(model, tokenizer, prefix, num_beams=4, max_tokens=20, merge=product):\n",
    "\n",
    "  inputs = tokenizer(prefix, return_tensors=\"pt\")\n",
    "  input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "  list_input_ids = input_ids.tolist()[0]\n",
    "  \n",
    "  # Beams are tuples ([tokens...], log_prob, score)\n",
    "  beams = [[list_input_ids,0,1]]\n",
    "\n",
    "  # List the possible next num_beams beams and scores that are children of the given beam\n",
    "  def next_beams(beam,k=10):\n",
    "    input_ids = beam[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "      outputs = model(torch.tensor([input_ids]).to(model.device))\n",
    "\n",
    "    predictions = F.softmax(outputs.logits, dim=-1).detach().cpu()\n",
    "\n",
    "    last_token_predictions = predictions[0][-1]\n",
    "\n",
    "    idxs = torch.argsort(last_token_predictions, descending=True)[0:k]\n",
    "\n",
    "    probs = last_token_predictions[idxs].tolist()\n",
    "\n",
    "    next_states = [tokenizer.decode(input_ids + [x.tolist()])[1:]  for x in idxs]\n",
    "    hs = [llama_hs_from_text(model, tokenizer, x) for x in next_states]\n",
    "\n",
    "    credence = classify_single(probes[0], hs)\n",
    "\n",
    "    if(probe0Reversed):\n",
    "      credence = 1-credence\n",
    "\n",
    "    def sigmoid(x):\n",
    "      return 1/(1 + np.exp(-1 * x))\n",
    "    truthfulness = [sigmoid(x) for x in credence]\n",
    "\n",
    "    scores = torch.tensor([merge(probs[i], truthfulness[i]) for i in range(k)])\n",
    "\n",
    "    resorted = torch.argsort(scores, descending=True)\n",
    "\n",
    "    # TODO\n",
    "    return [[input_ids + [idxs[i].item()], ] for i in resorted[0:num_beams]]\n",
    "\n",
    "\n",
    "  for i in range(max_tokens):\n",
    "    print(beams)\n",
    "    # Enumerate all children\n",
    "    next = [beam for prev in beams for beam in next_beams(prev)]\n",
    "    next = sorted(\n",
    "      next,\n",
    "      key=lambda x: x[1],\n",
    "      reverse=True\n",
    "    )\n",
    "    beams = next[0:num_beams]\n",
    "\n",
    "  return beams\n",
    "\n",
    "print(beam_search(model, tokenizer, \"Obama is\", merge=product))\n",
    "\n",
    "      \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M and Nasrabadi, Nasser M},
  volume={4},
  year={2006},
  publisher={Springer}
}

@misc{bubeck2023sparks,
      title={Sparks of Artificial General Intelligence: Early experiments with GPT-4}, 
      author={Sébastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},
      year={2023},
      eprint={2303.12712},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{perez2022discovering,
      title={Discovering Language Model Behaviors with Model-Written Evaluations}, 
      author={Ethan Perez and Sam Ringer and Kamilė Lukošiūtė and Karina Nguyen and Edwin Chen and Scott Heiner and Craig Pettit and Catherine Olsson and Sandipan Kundu and Saurav Kadavath and Andy Jones and Anna Chen and Ben Mann and Brian Israel and Bryan Seethor and Cameron McKinnon and Christopher Olah and Da Yan and Daniela Amodei and Dario Amodei and Dawn Drain and Dustin Li and Eli Tran-Johnson and Guro Khundadze and Jackson Kernion and James Landis and Jamie Kerr and Jared Mueller and Jeeyoon Hyun and Joshua Landau and Kamal Ndousse and Landon Goldberg and Liane Lovitt and Martin Lucas and Michael Sellitto and Miranda Zhang and Neerav Kingsland and Nelson Elhage and Nicholas Joseph and Noemí Mercado and Nova DasSarma and Oliver Rausch and Robin Larson and Sam McCandlish and Scott Johnston and Shauna Kravec and Sheer El Showk and Tamera Lanham and Timothy Telleen-Lawton and Tom Brown and Tom Henighan and Tristan Hume and Yuntao Bai and Zac Hatfield-Dodds and Jack Clark and Samuel R. Bowman and Amanda Askell and Roger Grosse and Danny Hernandez and Deep Ganguli and Evan Hubinger and Nicholas Schiefer and Jared Kaplan},
      year={2022},
      eprint={2212.09251},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lemons2022beam,
      title={Beam Search: Faster and Monotonic}, 
      author={Sofia Lemons and Carlos Linares López and Robert C. Holte and Wheeler Ruml},
      year={2022},
      eprint={2204.02929},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{beamsearch
title={Speech understanding systems: A summary of results of the five-year research effort. }
journal={}
author={Reddy}
}

@article{mcts
https://arxiv.org/pdf/2104.05336.pdf
journal={arXiv preprint arXiv:
}

@misc{separation,
title={LLM Modularity: The Separability of Capabilities in Large Language Models},
author={Nicky Pochinkov},
year={2023},
url={https://www.alignmentforum.org/posts/j84JhErNezMxyK4dH/llm-modularity-the-separability-of-capabilities-in-large}
}

https://www.alignmentforum.org/posts/j84JhErNezMxyK4dH/llm-modularity-the-separability-of-capabilities-in-large


@misc{eloundou2023gpts,
      title={GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models}, 
      author={Tyna Eloundou and Sam Manning and Pamela Mishkin and Daniel Rock},
      year={2023},
      eprint={2303.10130},
      archivePrefix={arXiv},
      primaryClass={econ.GN}
}

@misc{miao2018cgmh,
      title={CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling}, 
      author={Ning Miao and Hao Zhou and Lili Mou and Rui Yan and Lei Li},
      year={2018},
      eprint={1811.10996},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{stochasticparrots,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610–623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}


@article{vinc,
  title={VINC: Eliciting Latent Knowledge is Easier Than You Think},
  author={Belrose, Nora and Mallen, Alex and Laurito, Walter and Koh, Christy and Lee, Reagan and O
  Brien, Kyle and Wan, Alex and Wright, Ben},
  journal={preprint},
  year={2023}
}

@misc{ghojogh2020sampling,
      title={Sampling Algorithms, from Survey Sampling to Monte Carlo Methods: Tutorial and Literature Review}, 
      author={Benyamin Ghojogh and Hadi Nekoei and Aydin Ghojogh and Fakhri Karray and Mark Crowley},
      year={2020},
      eprint={2011.00901},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

@misc{rlhfsocial,
      title={Perspectives on the Social Impacts of Reinforcement Learning with Human Feedback}, 
      author={Gabrielle Kaili-May Liu},
      year={2023},
      eprint={2303.02891},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@article{botev2022regularising,
  title={Regularising for invariance to data augmentation improves supervised learning},
  author={Botev, Aleksander and Bauer, Matthias and De, Soham},
  journal={arXiv preprint arXiv:2203.03304},
  year={2022}
}

@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{ccs,
  title={Discovering latent knowledge in language models without supervision},
  author={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2212.03827},
  year={2022}
}

@techreport{christiano2021eliciting,
  title={Eliciting latent knowledge: How to tell if your eyes deceive you},
  author={Christiano, Paul and Cotra, Ajeya and Xu, Mark},
  institution={Alignment Research Center},
  year={2021},
  month={December},
  url={https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit}
}

@misc{wei2023chainofthought,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{christiano2022prize,
  author = {Paul Christiano and Mark Xu},
  title = {{ELK} prize results},
  howpublished = {\url{https://www.alignment.org/blog/elk-prize-results/}},
  year = {2022},
  month = {March 8},
}

@article{evans2021truthful,
  title={Truthful AI: Developing and governing AI that does not lie},
  author={Evans, Owain and Cotton-Barratt, Owen and Finnveden, Lukas and Bales, Adam and Balwit, Avital and Wills, Peter and Righetti, Luca and Saunders, William},
  journal={arXiv preprint arXiv:2110.06674},
  year={2021}
}

@misc{zhao2023survey,
      title={A Survey of Large Language Models}, 
      author={Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
      year={2023},
      eprint={2303.18223},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{loshchilov2019decoupled,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{lin2021truthfulqa,
  title={Truthfulqa: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}

@article{xie2020unsupervised,
  title={Unsupervised data augmentation for consistency training},
  author={Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Thang and Le, Quoc},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6256--6268},
  year={2020}
}